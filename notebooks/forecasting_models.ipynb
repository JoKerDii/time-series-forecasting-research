{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b25c52",
   "metadata": {},
   "source": [
    "## Computational Complexity Analysis\n",
    "\n",
    "| Model | Training Complexity | Prediction Complexity | Memory Usage | Data Level |\n",
    "|-------|-------------------|---------------------|--------------|------------|\n",
    "| Prophet | O(n log n) | O(1) | Low | Aggregated |\n",
    "| Random Forest | O(n log n × m × t) | O(log t) | Medium | Store-Dept |\n",
    "| LSTM | O(n × s × f × h²) | O(s × f × h²) | High | Store-Dept |\n",
    "| Transformer | O(n × s² × f) | O(s² × f) | Very High | Store Level |\n",
    "\n",
    "Where:\n",
    "- n = number of samples\n",
    "- m = number of features  \n",
    "- t = number of trees\n",
    "- s = sequence length\n",
    "- f = feature dimension\n",
    "- h = hidden units\n",
    "\n",
    "### Granularity vs Efficiency Trade-offs\n",
    "\n",
    "The system demonstrates a clear **granularity-efficiency spectrum**:\n",
    "\n",
    "**Granularity Hierarchy** (Most to Least Detailed):\n",
    "1. **LSTM & Random Forest**: Store-Department level (most granular)\n",
    "2. **Transformer**: Store level (departments aggregated)  \n",
    "3. **Prophet**: Total aggregated (all stores and departments combined)\n",
    "\n",
    "**Computational Efficiency** (Fastest to Slowest):\n",
    "1. **Prophet**: O(n log n) - fastest, simplest\n",
    "2. **Random Forest**: O(n log n × m × t) - fast, parallel trees\n",
    "3. **LSTM**: O(n × s × f × h²) - slow, sequential processing\n",
    "4. **Transformer**: O(n × s² × f) - slowest, quadratic attention\n",
    "\n",
    "**Feature Complexity Progression**:\n",
    "- **Transformer**: 5 features (computational efficiency priority)\n",
    "- **Prophet**: 7 regressors (economic focus)\n",
    "- **LSTM**: 15 features (sequential pattern optimization)\n",
    "- **Random Forest**: 20 features (maximum information utilization)\n",
    "\n",
    "This design allows the ensemble to capture patterns at multiple scales while balancing computational constraints.# Walmart Sales Forecasting Models - Technical Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `WalmartForecastingModels` class implements a comprehensive ensemble of state-of-the-art forecasting models specifically optimized for retail sales prediction. This system addresses the unique challenges of the Walmart Sales Forecasting competition, incorporating multiple advanced techniques including statistical methods, tree-based algorithms, and deep learning approaches.\n",
    "\n",
    "## Problem Context\n",
    "\n",
    "Walmart sales forecasting presents several distinct challenges:\n",
    "- **Seasonal patterns**: Weekly, monthly, and yearly seasonality\n",
    "- **Holiday effects**: Significant sales spikes during major holidays (5x normal weight)\n",
    "- **Store heterogeneity**: 45 stores with different characteristics and performance patterns\n",
    "- **Department variations**: 99 departments with unique sales behaviors\n",
    "- **External factors**: Economic indicators (unemployment, CPI, fuel prices) and weather impacts\n",
    "- **Promotional effects**: Markdown events that drive sales variations\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The system implements four complementary forecasting approaches:\n",
    "\n",
    "### 1. Prophet Model (Statistical Approach)\n",
    "\n",
    "**What it is:**\n",
    "Prophet is Facebook's time series forecasting tool designed for business applications with strong seasonal patterns and holiday effects.\n",
    "\n",
    "**Implementation Details:**\n",
    "- **Seasonality**: Multiplicative mode with yearly seasonality enabled\n",
    "- **Holiday handling**: Custom holiday calendar for major retail events (Super Bowl, Labor Day, Thanksgiving, Christmas)\n",
    "- **External regressors**: Temperature, fuel price, CPI, unemployment, and markdown totals\n",
    "- **Data aggregation**: Aggregates all store-department combinations by date for macro-level trends\n",
    "\n",
    "**Why it works for Walmart:**\n",
    "- Explicitly models holiday effects with custom prior scales (25x importance)\n",
    "- Handles missing data gracefully\n",
    "- Incorporates external economic factors as regressors\n",
    "- Robust to outliers and structural breaks\n",
    "\n",
    "**Strengths:**\n",
    "- Interpretable components (trend, seasonality, holidays)\n",
    "- Fast training and prediction\n",
    "- Built-in uncertainty quantification\n",
    "- Minimal hyperparameter tuning required\n",
    "\n",
    "**Weaknesses:**\n",
    "- Loses granular store-department level patterns through aggregation\n",
    "- Linear relationship assumptions for regressors\n",
    "- Less flexible than neural networks for complex interactions\n",
    "\n",
    "### 2. LSTM Model (Deep Learning - RNN)\n",
    "\n",
    "**What it is:**\n",
    "A deep Long Short-Term Memory network with holiday-aware architecture designed to capture sequential dependencies in sales data.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (sequence_length=5, features=15) \n",
    "→ LSTM(64, return_sequences=True) + Dropout(0.3)\n",
    "→ LSTM(32, return_sequences=True) + Dropout(0.2) \n",
    "→ LSTM(16) + Dropout(0.2)\n",
    "→ Dense(32, ReLU) → Dense(16, ReLU) → Dense(1)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **Sequence modeling**: Uses 5-week lookback windows\n",
    "- **Feature selection**: Top 15 features based on correlation analysis\n",
    "- **Holiday weighting**: Sample weights applied during training (5x for holiday weeks)\n",
    "- **Store-department level**: Maintains granular predictions\n",
    "- **Scaling**: StandardScaler normalization for stable training\n",
    "\n",
    "**Why it works for Walmart:**\n",
    "- Captures temporal dependencies and sequential patterns\n",
    "- Memory cells retain long-term seasonal information\n",
    "- Multiple LSTM layers learn hierarchical patterns\n",
    "- Holiday weighting emphasizes critical sales periods\n",
    "\n",
    "**Strengths:**\n",
    "- Excellent for sequential pattern recognition\n",
    "- Handles variable-length sequences\n",
    "- Captures long-term dependencies\n",
    "- Non-linear feature interactions\n",
    "\n",
    "**Weaknesses:**\n",
    "- High computational complexity O(n×m×h²) where n=samples, m=features, h=hidden units\n",
    "- Requires large amounts of training data\n",
    "- Black box model with limited interpretability\n",
    "- Sensitive to hyperparameter choices\n",
    "\n",
    "### 3. Transformer Model (Attention-Based)\n",
    "\n",
    "**What it is:**\n",
    "A Transformer architecture adapted for time series forecasting using multi-head self-attention mechanisms.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (sequence_length=5, features=5)\n",
    "→ MultiHeadAttention(heads=4, key_dim=32) + LayerNorm\n",
    "→ Dense(128, ReLU) + Dropout(0.2) → Dense(features) + LayerNorm  \n",
    "→ GlobalAveragePooling1D → Dense(64, ReLU) + Dropout(0.3) → Dense(1)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **Self-attention**: Learns relationships between different time steps\n",
    "- **Store-level aggregation**: Reduces computational complexity while preserving temporal patterns\n",
    "- **Multi-head attention**: 4 attention heads capture different pattern types\n",
    "- **Feed-forward networks**: Dense layers for non-linear transformations\n",
    "\n",
    "**Why it works for Walmart:**\n",
    "- Attention mechanism identifies relevant historical periods\n",
    "- Parallel processing of temporal sequences\n",
    "- Captures both local and global temporal dependencies\n",
    "- Less prone to vanishing gradients than RNNs\n",
    "\n",
    "**Strengths:**\n",
    "- Superior long-range dependency modeling\n",
    "- Parallel computation (faster than RNNs)\n",
    "- Attention weights provide some interpretability\n",
    "- State-of-the-art performance on many sequence tasks\n",
    "\n",
    "**Weaknesses:**\n",
    "- High memory requirements O(n²) for sequence length n\n",
    "- Complex architecture requires careful tuning\n",
    "- May overfit on small datasets\n",
    "- Computational intensity limits scalability\n",
    "\n",
    "### 4. Random Forest Model (Tree-Based Baseline)\n",
    "\n",
    "**What it is:**\n",
    "An ensemble of decision trees with Walmart-specific feature engineering and holiday sample weighting.\n",
    "\n",
    "**Configuration:**\n",
    "- **Estimators**: 100 trees\n",
    "- **Max depth**: 15 levels\n",
    "- **Sample weighting**: Holiday weeks weighted 5x during training\n",
    "- **Features**: Top 20 most important features\n",
    "- **Regularization**: min_samples_split=10, min_samples_leaf=5\n",
    "\n",
    "**Why it works for Walmart:**\n",
    "- Handles mixed data types (categorical and numerical)\n",
    "- Built-in feature importance ranking\n",
    "- Robust to outliers and missing values\n",
    "- Non-linear relationships without explicit feature engineering\n",
    "\n",
    "**Strengths:**\n",
    "- Fast training and prediction\n",
    "- Feature importance interpretation\n",
    "- Handles missing values naturally\n",
    "- No scaling requirements\n",
    "- Robust to outliers\n",
    "\n",
    "**Weaknesses:**\n",
    "- Limited extrapolation capability\n",
    "- Can overfit with deep trees\n",
    "- Biased toward features with more levels\n",
    "- Struggles with linear relationships\n",
    "\n",
    "## Data Handling Strategies\n",
    "\n",
    "### Time Series Splitting\n",
    "- **Validation approach**: Time-based split using last 8 weeks\n",
    "- **Rationale**: Prevents data leakage and mimics real-world deployment\n",
    "- **Store-department integrity**: Maintains temporal order within each store-department combination\n",
    "\n",
    "### Holiday Weighting System\n",
    "```python\n",
    "Holiday_Weight = 5.0 if IsHoliday else 1.0\n",
    "```\n",
    "- Applied during model training to emphasize holiday performance\n",
    "- Addresses class imbalance (few holiday weeks vs. many regular weeks)\n",
    "- Critical for competition metric optimization\n",
    "\n",
    "### Model-Specific Data Structures\n",
    "\n",
    "Each model requires fundamentally different data formatting due to their unique architectures:\n",
    "\n",
    "#### Prophet Model Data Structure\n",
    "**Format**: Aggregated time series\n",
    "```python\n",
    "# Aggregates all store-department combinations by date\n",
    "prophet_train = train_data.groupby(\"Date\").agg({\n",
    "    \"Weekly_Sales\": \"sum\",        # Total sales across all stores/depts\n",
    "    \"Temperature\": \"mean\",        # Average temperature\n",
    "    \"Fuel_Price\": \"mean\",        # Average fuel price\n",
    "    \"CPI\": \"mean\",               # Average CPI\n",
    "    \"Unemployment\": \"mean\",      # Average unemployment\n",
    "    \"IsHoliday\": \"max\",          # Holiday indicator\n",
    "    \"Total_MarkDown\": \"sum\"      # Total markdowns\n",
    "})\n",
    "\n",
    "# Prophet-required column names\n",
    "columns = [\"ds\", \"y\", \"temperature\", \"fuel_price\", \"cpi\", \"unemployment\", \"holiday\", \"markdown\"]\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- **Single time series**: One aggregated series instead of multiple combinations\n",
    "- **Shape**: `(n_weeks, 8_columns)` - one row per week\n",
    "- **External regressors**: Economic and weather variables\n",
    "- **Custom holidays**: Manually defined retail holidays\n",
    "\n",
    "#### Random Forest Data Structure\n",
    "**Format**: Flat tabular structure\n",
    "```python\n",
    "# Standard ML format with features as columns\n",
    "feature_cols = [\n",
    "    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
    "    'IsHoliday', 'Total_MarkDown', 'Month_sin', 'Month_cos',\n",
    "    'Sales_lag_1', 'Sales_rolling_mean_4', 'Store_Size_encoded'\n",
    "    # ... up to 20 features total\n",
    "]\n",
    "\n",
    "X_train = train_data[feature_cols].fillna(0)      # Shape: (n_samples, 20)\n",
    "y_train = train_data[\"Weekly_Sales\"]              # Shape: (n_samples,)\n",
    "sample_weights = train_data[\"Holiday_Weight\"]     # Shape: (n_samples,)\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- **Granular level**: Store-department-week observations\n",
    "- **Shape**: `(n_store_dept_weeks, 20_features)`\n",
    "- **Sample weighting**: Holiday weeks weighted 5x during training\n",
    "- **No aggregation**: Preserves all granular patterns\n",
    "\n",
    "#### LSTM Data Structure\n",
    "**Format**: 3D sequential tensors\n",
    "```python\n",
    "# Creates time sequences for each store-department combination\n",
    "def create_sequences_walmart(data, seq_length=5):\n",
    "    X, y, weights = [], [], []\n",
    "    \n",
    "    for (store, dept), group in data.groupby(['Store', 'Dept']):\n",
    "        group_sorted = group.sort_values('Date')\n",
    "        \n",
    "        for i in range(seq_length, len(group_sorted)):\n",
    "            # 5 weeks of feature data\n",
    "            X.append(group_sorted[features].iloc[i-5:i].values)  # Shape: (5, 15)\n",
    "            # Next week's sales\n",
    "            y.append(group_sorted['Weekly_Sales'].iloc[i])\n",
    "            # Holiday weight\n",
    "            weights.append(group_sorted['Holiday_Weight'].iloc[i])\n",
    "\n",
    "# Final scaled shapes\n",
    "X_train_scaled.shape  # (n_sequences, 5, 15) - 3D tensor\n",
    "y_train_scaled.shape  # (n_sequences,) - 1D array\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- **3D structure**: `(samples, timesteps, features)` = `(n_sequences, 5, 15)`\n",
    "- **Temporal sequences**: 5 consecutive weeks per sample\n",
    "- **Store-department level**: Separate sequences for each combination\n",
    "- **Feature selection**: Top 15 most important features\n",
    "\n",
    "#### Transformer Data Structure\n",
    "**Format**: Store-level aggregated sequences\n",
    "```python\n",
    "# Store-level aggregation (departments combined)\n",
    "agg_train = train_data.groupby(['Store', 'Date']).agg({\n",
    "    'Weekly_Sales': 'sum',        # Total sales per store\n",
    "    'Temperature': 'mean',        \n",
    "    'Fuel_Price': 'mean',         \n",
    "    'Unemployment': 'mean',       \n",
    "    'IsHoliday': 'max',          \n",
    "    'Total_MarkDown': 'sum'      \n",
    "})\n",
    "\n",
    "# Simplified feature set (5 features only)\n",
    "features = ['Temperature', 'Fuel_Price', 'Unemployment', 'IsHoliday', 'Total_MarkDown']\n",
    "\n",
    "# Final shapes\n",
    "X_train_scaled.shape  # (n_sequences, 5, 5) - 3D tensor\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- **Store-level aggregation**: Combines departments within stores\n",
    "- **Reduced features**: Only 5 core features vs 15 for LSTM\n",
    "- **3D structure**: `(samples, timesteps, features)` = `(n_sequences, 5, 5)`\n",
    "- **Computational efficiency**: Simplified for faster attention computation\n",
    "\n",
    "### Data Structure Comparison Table\n",
    "\n",
    "| Model | Data Level | Structure | Shape | Features | Rationale |\n",
    "|-------|------------|-----------|-------|----------|-----------|\n",
    "| **Prophet** | All Stores Aggregated | 2D Time Series | `(weeks, 8)` | 7 regressors | Macro-level trends, interpretability |\n",
    "| **Random Forest** | Store-Dept Level | 2D Tabular | `(observations, 20)` | 20 engineered | Maximum granularity, feature importance |\n",
    "| **LSTM** | Store-Dept Level | 3D Sequences | `(sequences, 5, 15)` | 15 selected | Sequential patterns, dept-level detail |\n",
    "| **Transformer** | Store Level | 3D Sequences | `(sequences, 5, 5)` | 5 core | Attention efficiency, store-level focus |\n",
    "\n",
    "### Feature Complexity Trade-offs\n",
    "\n",
    "**Why Transformer Uses Fewer Features (5 vs 15 for LSTM)**:\n",
    "\n",
    "1. **Computational Complexity**: Transformer attention mechanism has quadratic complexity with feature dimensions\n",
    "2. **Store-level Aggregation**: Department-specific features become less meaningful when aggregated\n",
    "3. **Attention Benefits**: Self-attention learns feature interactions automatically, reducing need for engineered features  \n",
    "4. **Memory Constraints**: Fewer features = smaller attention matrices = more efficient training\n",
    "5. **Stable Features**: Focuses on economic indicators and business factors that work well across stores\n",
    "\n",
    "### Sequence Creation (Neural Networks)\n",
    "- **LSTM**: 5-week sequences per store-department combination\n",
    "- **Transformer**: Store-level sequences with department aggregation\n",
    "- **Padding strategy**: Skip insufficient sequences rather than padding\n",
    "- **Scaling**: Feature-wise standardization for neural network stability\n",
    "\n",
    "### Data Flow Pipeline\n",
    "1. **Raw Data**: Store-Dept-Week level observations\n",
    "2. **Feature Engineering**: Add lags, rolling statistics, cyclical encodings\n",
    "3. **Model-Specific Preparation**:\n",
    "   - **Prophet**: Aggregate to weekly totals → 2D time series\n",
    "   - **Random Forest**: Keep original structure → 2D matrix  \n",
    "   - **LSTM**: Create store-dept sequences → 3D tensor (15 features)\n",
    "   - **Transformer**: Aggregate to store level → create sequences → 3D tensor (5 features)\n",
    "\n",
    "### Granularity vs Efficiency Trade-offs\n",
    "\n",
    "The system demonstrates a clear **granularity-efficiency spectrum**:\n",
    "\n",
    "**Granularity Hierarchy** (Most to Least Detailed):\n",
    "1. **LSTM & Random Forest**: Store-Department level (most granular)\n",
    "2. **Transformer**: Store level (departments aggregated)  \n",
    "3. **Prophet**: Total aggregated (all stores and departments combined)\n",
    "\n",
    "**Computational Efficiency** (Fastest to Slowest):\n",
    "1. **Prophet**: O(n log n) - fastest, simplest\n",
    "2. **Random Forest**: O(n log n × m × t) - fast, parallel trees\n",
    "3. **LSTM**: O(n × s × f × h²) - slow, sequential processing\n",
    "4. **Transformer**: O(n × s² × f) - slowest, quadratic attention\n",
    "\n",
    "**Feature Complexity Progression**:\n",
    "- **Transformer**: 5 features (computational efficiency priority)\n",
    "- **Prophet**: 7 regressors (economic focus)\n",
    "- **LSTM**: 15 features (sequential pattern optimization)\n",
    "- **Random Forest**: 20 features (maximum information utilization)\n",
    "\n",
    "This design allows the ensemble to capture patterns at multiple scales while balancing computational constraints.\n",
    "\n",
    "## Model Selection and Ensemble Strategy\n",
    "\n",
    "### Individual Model Use Cases\n",
    "\n",
    "**Prophet**: Best for\n",
    "- Quick baseline establishment\n",
    "- Business stakeholder communication (interpretable components)\n",
    "- Long-term trend analysis\n",
    "- Scenarios with limited computational resources\n",
    "\n",
    "**Random Forest**: Best for\n",
    "- Feature importance analysis\n",
    "- Robust baseline performance\n",
    "- Mixed data types handling\n",
    "- When interpretability is required\n",
    "\n",
    "**LSTM**: Best for\n",
    "- Complex temporal pattern recognition\n",
    "- When sequential dependencies are critical\n",
    "- Non-linear feature interactions\n",
    "- Sufficient training data availability\n",
    "\n",
    "**Transformer**: Best for\n",
    "- State-of-the-art performance requirements\n",
    "- Long-range dependency modeling\n",
    "- When computational resources are abundant\n",
    "- Complex attention pattern analysis\n",
    "\n",
    "### Ensemble Considerations\n",
    "The models complement each other:\n",
    "- **Prophet** captures macro trends and seasonality\n",
    "- **Random Forest** provides robust baseline and feature insights\n",
    "- **LSTM** models sequential dependencies at granular level\n",
    "- **Transformer** captures complex attention patterns\n",
    "\n",
    "## Performance Optimization Strategies\n",
    "\n",
    "### Neural Network Optimizations\n",
    "- **Early stopping**: Prevents overfitting (patience=8-10)\n",
    "- **Learning rate scheduling**: ReduceLROnPlateau for convergence\n",
    "- **Dropout layers**: Regularization to prevent overfitting\n",
    "- **Batch size optimization**: 32-64 for memory efficiency\n",
    "\n",
    "### Feature Selection\n",
    "- **Correlation-based**: Top features by correlation with target\n",
    "- **Critical feature enforcement**: Ensures important features included\n",
    "- **Dimensionality management**: Limits features to prevent curse of dimensionality\n",
    "\n",
    "### Memory Management\n",
    "- **Data aggregation**: Transformer uses store-level rather than store-department level\n",
    "- **Sequence batching**: Processes sequences in manageable batches\n",
    "- **Model checkpointing**: Saves best weights during training\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "### Data Requirements\n",
    "- **Minimum sequence length**: Neural networks require sufficient historical data\n",
    "- **Holiday representation**: Limited holiday examples may affect generalization\n",
    "- **Store-department combinations**: Some combinations have insufficient data\n",
    "\n",
    "### Scalability Constraints\n",
    "- **Neural networks**: Memory and computation scale poorly with large datasets\n",
    "- **Real-time deployment**: LSTM and Transformer have higher latency\n",
    "- **Storage requirements**: Multiple models increase storage needs\n",
    "\n",
    "### Generalization Challenges\n",
    "- **Walmart-specific**: Heavy customization may not transfer to other retailers\n",
    "- **Holiday calendar**: Hard-coded holidays may not apply to other regions\n",
    "- **Economic indicators**: External factors may not be available in all contexts\n",
    "\n",
    "## Best Practices and Recommendations\n",
    "\n",
    "### Model Development\n",
    "1. **Start with Prophet** for quick insights and baseline\n",
    "2. **Use Random Forest** for feature importance analysis\n",
    "3. **Deploy LSTM** when sequential patterns are critical\n",
    "4. **Consider Transformer** only with sufficient computational resources\n",
    "\n",
    "### Production Deployment\n",
    "1. **Ensemble voting**: Combine predictions from multiple models\n",
    "2. **A/B testing**: Compare model performance in production\n",
    "3. **Monitoring**: Track prediction accuracy and model drift\n",
    "4. **Retraining schedule**: Regular model updates with new data\n",
    "\n",
    "### Performance Tuning\n",
    "1. **Cross-validation**: Use time series cross-validation for hyperparameter tuning\n",
    "2. **Feature engineering**: Continuously evaluate and add relevant features\n",
    "3. **Architecture search**: Experiment with different neural network architectures\n",
    "4. **Regularization**: Balance model complexity with generalization\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This Walmart forecasting system represents a comprehensive approach to retail sales prediction, combining the strengths of statistical, tree-based, and deep learning methods. The holiday weighting system and Walmart-specific feature engineering make it particularly well-suited for retail forecasting challenges. While the neural network approaches offer superior performance potential, the statistical and tree-based methods provide valuable baselines and interpretability. The choice of model should be based on specific requirements for accuracy, interpretability, computational resources, and deployment constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All datasets loaded successfully!\n",
      "=== MERGING DATASETS ===\n",
      "Merged training data shape: (421570, 16)\n",
      "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
      "=== WALMART-SPECIFIC FEATURE ENGINEERING ===\n",
      "Feature engineering completed. New shape: (421570, 67)\n",
      "Added 62 new features\n",
      "=== HANDLING MISSING VALUES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:44:11 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled. Remaining NaN: 0\n",
      "Feature Engineering class ready!\n",
      "=== PREPARING WALMART DATA FOR MODELING ===\n",
      "Training data: (397841, 67)\n",
      "Validation data: (23729, 67)\n",
      "Features: 62\n",
      "Holiday weeks in training: 26695\n",
      "=== TRAINING WALMART-OPTIMIZED PROPHET MODEL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:44:11 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model trained in 0.14 seconds\n",
      "=== TRAINING WALMART LSTM MODEL ===\n",
      "WARNING:tensorflow:5 out of the last 278 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x417f8f880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "LSTM model trained in 198.94 seconds\n",
      "Using 19 features\n",
      "=== TRAINING WALMART RANDOM FOREST MODEL ===\n",
      "Random Forest model trained in 4.09 seconds\n",
      "Top 5 most important features:\n",
      "         feature  importance\n",
      "10          Size    0.658443\n",
      "9   Unemployment    0.079670\n",
      "8            CPI    0.074539\n",
      "16          Week    0.056932\n",
      "5      MarkDown3    0.031259\n",
      "=== TRAINING WALMART TRANSFORMER MODEL ===\n",
      "Transformer model trained in 10.54 seconds\n",
      "Forecasting Models class ready!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GRU,\n",
    "    Input,\n",
    "    MultiHeadAttention,\n",
    "    LayerNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "class WalmartForecastingModels:\n",
    "    \"\"\"Forecasting models optimized for Walmart competition\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.feature_columns = []\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "\n",
    "    def prepare_walmart_data(self, validation_weeks=8):\n",
    "        \"\"\"Prepare data specifically for Walmart forecasting with holiday weights\"\"\"\n",
    "        print(\"=== PREPARING WALMART DATA FOR MODELING ===\")\n",
    "\n",
    "        # Remove rows without sales data (early periods with insufficient lags)\n",
    "        self.data_clean = self.data.dropna(subset=[\"Weekly_Sales\"])\n",
    "\n",
    "        # Sort by date for proper time series split\n",
    "        self.data_clean = self.data_clean.sort_values([\"Store\", \"Dept\", \"Date\"])\n",
    "\n",
    "        # Select feature columns\n",
    "        exclude_cols = [\"Weekly_Sales\", \"Store\", \"Dept\", \"Date\", \"Type\"]\n",
    "        self.feature_columns = [\n",
    "            col\n",
    "            for col in self.data_clean.columns\n",
    "            if col not in exclude_cols and not col.endswith(\"_scaled\")\n",
    "        ]\n",
    "\n",
    "        # Time-based split (last N weeks for validation)\n",
    "        unique_dates = sorted(self.data_clean[\"Date\"].unique())\n",
    "        split_date = unique_dates[-validation_weeks]\n",
    "\n",
    "        self.train_data = self.data_clean[self.data_clean[\"Date\"] < split_date].copy()\n",
    "        self.val_data = self.data_clean[self.data_clean[\"Date\"] >= split_date].copy()\n",
    "\n",
    "        # Create holiday weights for training data\n",
    "        self.train_weights = self.train_data[\"Holiday_Weight\"].values\n",
    "        self.val_weights = self.val_data[\"Holiday_Weight\"].values\n",
    "\n",
    "        print(f\"Training data: {self.train_data.shape}\")\n",
    "        print(f\"Validation data: {self.val_data.shape}\")\n",
    "        print(f\"Features: {len(self.feature_columns)}\")\n",
    "        print(f\"Holiday weeks in training: {(self.train_weights == 5.0).sum()}\")\n",
    "\n",
    "        return self.train_data, self.val_data\n",
    "\n",
    "    def prophet_walmart_model(self):\n",
    "        \"\"\"Prophet model optimized for Walmart data\"\"\"\n",
    "        print(\"=== TRAINING WALMART-OPTIMIZED PROPHET MODEL ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Aggregate by date for Prophet (total sales across all stores/depts)\n",
    "            prophet_train = (\n",
    "                self.train_data.groupby(\"Date\")\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"CPI\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                        \"Total_MarkDown\": \"sum\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            prophet_train.columns = [\n",
    "                \"ds\",\n",
    "                \"y\",\n",
    "                \"temperature\",\n",
    "                \"fuel_price\",\n",
    "                \"cpi\",\n",
    "                \"unemployment\",\n",
    "                \"holiday\",\n",
    "                \"markdown\",\n",
    "            ]\n",
    "\n",
    "            # Initialize Prophet with Walmart-specific settings\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=False,  # Weekly data, not daily\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode=\"multiplicative\",\n",
    "                changepoint_prior_scale=0.1,  # More flexible for retail\n",
    "                seasonality_prior_scale=15,  # Strong seasonality in retail\n",
    "                holidays_prior_scale=25,  # Strong holiday effects\n",
    "                interval_width=0.95,\n",
    "            )\n",
    "\n",
    "            # Add regressors\n",
    "            regressors = [\n",
    "                \"temperature\",\n",
    "                \"fuel_price\",\n",
    "                \"cpi\",\n",
    "                \"unemployment\",\n",
    "                \"markdown\",\n",
    "            ]\n",
    "            for regressor in regressors:\n",
    "                model.add_regressor(regressor)\n",
    "\n",
    "            # Add custom holidays (major retail holidays)\n",
    "            holidays = pd.DataFrame(\n",
    "                {\n",
    "                    \"holiday\": [\"Super Bowl\", \"Labor Day\", \"Thanksgiving\", \"Christmas\"]\n",
    "                    * 3,\n",
    "                    \"ds\": [\n",
    "                        \"2010-02-12\",\n",
    "                        \"2010-09-10\",\n",
    "                        \"2010-11-26\",\n",
    "                        \"2010-12-31\",\n",
    "                        \"2011-02-11\",\n",
    "                        \"2011-09-09\",\n",
    "                        \"2011-11-25\",\n",
    "                        \"2011-12-30\",\n",
    "                        \"2012-02-10\",\n",
    "                        \"2012-09-07\",\n",
    "                        \"2012-11-23\",\n",
    "                        \"2012-12-28\",\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "            holidays[\"ds\"] = pd.to_datetime(holidays[\"ds\"])\n",
    "            model.holidays = holidays\n",
    "\n",
    "            # Fit model\n",
    "            model.fit(prophet_train)\n",
    "\n",
    "            # Prepare validation data\n",
    "            prophet_val = (\n",
    "                self.val_data.groupby(\"Date\")\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"CPI\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                        \"Total_MarkDown\": \"sum\",\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            prophet_val.columns = [\n",
    "                \"ds\",\n",
    "                \"temperature\",\n",
    "                \"fuel_price\",\n",
    "                \"cpi\",\n",
    "                \"unemployment\",\n",
    "                \"holiday\",\n",
    "                \"markdown\",\n",
    "                \"actual\",\n",
    "            ]\n",
    "\n",
    "            # Make predictions\n",
    "            forecast = model.predict(prophet_val[[\"ds\"] + regressors])\n",
    "\n",
    "            # Extract predictions\n",
    "            predictions = forecast[\"yhat\"].values\n",
    "            actual = prophet_val[\"actual\"].values\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            self.models[\"Prophet\"] = model\n",
    "            self.results[\"Prophet\"] = {\n",
    "                \"predictions\": predictions,\n",
    "                \"actual\": actual,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Statistical\",\n",
    "                \"forecast\": forecast,\n",
    "                \"weights\": np.ones(\n",
    "                    len(predictions)\n",
    "                ),  # Simplified weights for aggregated data\n",
    "            }\n",
    "\n",
    "            print(f\"Prophet model trained in {training_time:.2f} seconds\")\n",
    "            return model, predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Prophet training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def lstm_walmart_model(self, sequence_length=5, epochs=50):\n",
    "        \"\"\"LSTM model with holiday weighting for Walmart data\"\"\"\n",
    "        print(\"=== TRAINING WALMART LSTM MODEL ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Select top features for LSTM\n",
    "            feature_importance = self._calculate_feature_importance()\n",
    "            top_features = feature_importance.head(15).index.tolist()\n",
    "\n",
    "            # Ensure critical features are included\n",
    "            critical_features = [\n",
    "                \"IsHoliday\",\n",
    "                \"Total_MarkDown\",\n",
    "                \"Month_sin\",\n",
    "                \"Month_cos\",\n",
    "                \"Sales_lag_1\",\n",
    "                \"Sales_rolling_mean_4\",\n",
    "            ]\n",
    "            for feat in critical_features:\n",
    "                if feat in self.train_data.columns and feat not in top_features:\n",
    "                    top_features.append(feat)\n",
    "\n",
    "            # Create sequences for time series\n",
    "            def create_sequences_walmart(\n",
    "                data, seq_length, features, target=\"Weekly_Sales\"\n",
    "            ):\n",
    "                X, y, weights = [], [], []\n",
    "\n",
    "                # Process by store-department combinations\n",
    "                for (store, dept), group in data.groupby([\"Store\", \"Dept\"]):\n",
    "                    if len(group) < seq_length + 1:\n",
    "                        continue\n",
    "\n",
    "                    group_sorted = group.sort_values(\"Date\")\n",
    "\n",
    "                    for i in range(seq_length, len(group_sorted)):\n",
    "                        # Features sequence\n",
    "                        X.append(group_sorted[features].iloc[i - seq_length : i].values)\n",
    "                        # Target\n",
    "                        y.append(group_sorted[target].iloc[i])\n",
    "                        # Holiday weight\n",
    "                        weights.append(group_sorted[\"Holiday_Weight\"].iloc[i])\n",
    "\n",
    "                return np.array(X), np.array(y), np.array(weights)\n",
    "\n",
    "            # Create training sequences\n",
    "            X_train, y_train, train_weights = create_sequences_walmart(\n",
    "                self.train_data, sequence_length, top_features\n",
    "            )\n",
    "\n",
    "            # Create validation sequences\n",
    "            X_val, y_val, val_weights = create_sequences_walmart(\n",
    "                self.val_data, sequence_length, top_features\n",
    "            )\n",
    "\n",
    "            if len(X_train) == 0 or len(X_val) == 0:\n",
    "                print(\"Insufficient data for LSTM sequences\")\n",
    "                return None, None\n",
    "\n",
    "            # Scale features\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(\n",
    "                X_train.reshape(-1, X_train.shape[-1])\n",
    "            ).reshape(X_train.shape)\n",
    "\n",
    "            X_val_scaled = scaler_X.transform(\n",
    "                X_val.reshape(-1, X_val.shape[-1])\n",
    "            ).reshape(X_val.shape)\n",
    "\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Build LSTM with holiday-aware architecture\n",
    "            model = Sequential(\n",
    "                [\n",
    "                    LSTM(\n",
    "                        64,\n",
    "                        return_sequences=True,\n",
    "                        input_shape=(sequence_length, len(top_features)),\n",
    "                    ),\n",
    "                    Dropout(0.3),\n",
    "                    LSTM(32, return_sequences=True),\n",
    "                    Dropout(0.2),\n",
    "                    LSTM(16, return_sequences=False),\n",
    "                    Dropout(0.2),\n",
    "                    Dense(32, activation=\"relu\"),\n",
    "                    Dense(16, activation=\"relu\"),\n",
    "                    Dense(1),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"]\n",
    "            )\n",
    "\n",
    "            # Train with callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_scaled,\n",
    "                validation_data=(\n",
    "                    X_val_scaled,\n",
    "                    scaler_y.transform(y_val.reshape(-1, 1)).flatten(),\n",
    "                ),\n",
    "                epochs=epochs,\n",
    "                batch_size=64,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred_scaled = model.predict(X_val_scaled, verbose=0)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            self.models[\"LSTM\"] = model\n",
    "            self.results[\"LSTM\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val,\n",
    "                \"weights\": val_weights,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Neural Network\",\n",
    "                \"history\": history,\n",
    "                \"scalers\": {\"X\": scaler_X, \"y\": scaler_y},\n",
    "                \"features\": top_features,\n",
    "            }\n",
    "\n",
    "            print(f\"LSTM model trained in {training_time:.2f} seconds\")\n",
    "            print(f\"Using {len(top_features)} features\")\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"LSTM training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def transformer_walmart_model(self, sequence_length=5, epochs=40):\n",
    "        \"\"\"Transformer model for Walmart time series\"\"\"\n",
    "        print(\"=== TRAINING WALMART TRANSFORMER MODEL ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Use aggregated data for transformer (computational efficiency)\n",
    "            agg_train = (\n",
    "                self.train_data.groupby([\"Store\", \"Date\"])\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                        \"Total_MarkDown\": \"sum\",\n",
    "                        \"Holiday_Weight\": \"max\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            agg_val = (\n",
    "                self.val_data.groupby([\"Store\", \"Date\"])\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                        \"Total_MarkDown\": \"sum\",\n",
    "                        \"Holiday_Weight\": \"max\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            # Prepare features\n",
    "            features = [\n",
    "                \"Temperature\",\n",
    "                \"Fuel_Price\",\n",
    "                \"Unemployment\",\n",
    "                \"IsHoliday\",\n",
    "                \"Total_MarkDown\",\n",
    "            ]\n",
    "\n",
    "            # Create sequences\n",
    "            def create_transformer_sequences(\n",
    "                data, seq_len, features, target=\"Weekly_Sales\"\n",
    "            ):\n",
    "                X, y, weights = [], [], []\n",
    "\n",
    "                for store in data[\"Store\"].unique():\n",
    "                    store_data = data[data[\"Store\"] == store].sort_values(\"Date\")\n",
    "\n",
    "                    if len(store_data) < seq_len + 1:\n",
    "                        continue\n",
    "\n",
    "                    for i in range(seq_len, len(store_data)):\n",
    "                        X.append(store_data[features].iloc[i - seq_len : i].values)\n",
    "                        y.append(store_data[target].iloc[i])\n",
    "                        weights.append(store_data[\"Holiday_Weight\"].iloc[i])\n",
    "\n",
    "                return np.array(X), np.array(y), np.array(weights)\n",
    "\n",
    "            X_train, y_train, train_weights = create_transformer_sequences(\n",
    "                agg_train, sequence_length, features\n",
    "            )\n",
    "            X_val, y_val, val_weights = create_transformer_sequences(\n",
    "                agg_val, sequence_length, features\n",
    "            )\n",
    "\n",
    "            if len(X_train) == 0 or len(X_val) == 0:\n",
    "                print(\"Insufficient data for Transformer\")\n",
    "                return None, None\n",
    "\n",
    "            # Scale data\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(\n",
    "                X_train.reshape(-1, X_train.shape[-1])\n",
    "            ).reshape(X_train.shape)\n",
    "\n",
    "            X_val_scaled = scaler_X.transform(\n",
    "                X_val.reshape(-1, X_val.shape[-1])\n",
    "            ).reshape(X_val.shape)\n",
    "\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Build Transformer model\n",
    "            inputs = Input(shape=(sequence_length, len(features)))\n",
    "\n",
    "            # Multi-head attention\n",
    "            attention_output = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.1)(\n",
    "                inputs, inputs\n",
    "            )\n",
    "\n",
    "            # Add & Norm\n",
    "            attention_output = LayerNormalization()(inputs + attention_output)\n",
    "\n",
    "            # Feed Forward Network\n",
    "            ffn_output = Dense(128, activation=\"relu\")(attention_output)\n",
    "            ffn_output = Dropout(0.2)(ffn_output)\n",
    "            ffn_output = Dense(len(features))(ffn_output)\n",
    "\n",
    "            # Add & Norm\n",
    "            ffn_output = LayerNormalization()(attention_output + ffn_output)\n",
    "\n",
    "            # Global pooling and output\n",
    "            pooled = GlobalAveragePooling1D()(ffn_output)\n",
    "            pooled = Dense(64, activation=\"relu\")(pooled)\n",
    "            pooled = Dropout(0.3)(pooled)\n",
    "            outputs = Dense(1)(pooled)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"]\n",
    "            )\n",
    "\n",
    "            # Train model\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=8, restore_best_weights=True\n",
    "                ),\n",
    "                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4),\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_scaled,\n",
    "                validation_data=(\n",
    "                    X_val_scaled,\n",
    "                    scaler_y.transform(y_val.reshape(-1, 1)).flatten(),\n",
    "                ),\n",
    "                epochs=epochs,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred_scaled = model.predict(X_val_scaled, verbose=0)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            self.models[\"Transformer\"] = model\n",
    "            self.results[\"Transformer\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val,\n",
    "                \"weights\": val_weights,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Neural Network\",\n",
    "                \"history\": history,\n",
    "            }\n",
    "\n",
    "            print(f\"Transformer model trained in {training_time:.2f} seconds\")\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Transformer training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def random_forest_walmart_model(self):\n",
    "        \"\"\"Random Forest baseline with Walmart-specific features\"\"\"\n",
    "        print(\"=== TRAINING WALMART RANDOM FOREST MODEL ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Prepare features\n",
    "            feature_cols = [\n",
    "                col for col in self.feature_columns if col in self.train_data.columns\n",
    "            ][\n",
    "                :20\n",
    "            ]  # Top 20 features\n",
    "\n",
    "            X_train = self.train_data[feature_cols].fillna(0)\n",
    "            y_train = self.train_data[\"Weekly_Sales\"]\n",
    "            X_val = self.val_data[feature_cols].fillna(0)\n",
    "            y_val = self.val_data[\"Weekly_Sales\"]\n",
    "\n",
    "            # Train Random Forest with holiday sample weights\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "            # Fit with sample weights (holiday weeks weighted 5x)\n",
    "            sample_weights = self.train_data[\"Holiday_Weight\"].values\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_val)\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Feature importance\n",
    "            feature_importance = pd.DataFrame(\n",
    "                {\"feature\": feature_cols, \"importance\": model.feature_importances_}\n",
    "            ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "            self.models[\"RandomForest\"] = model\n",
    "            self.results[\"RandomForest\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val.values,\n",
    "                \"weights\": self.val_data[\"Holiday_Weight\"].values,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Tree-based\",\n",
    "                \"feature_importance\": feature_importance,\n",
    "            }\n",
    "\n",
    "            print(f\"Random Forest model trained in {training_time:.2f} seconds\")\n",
    "            print(\"Top 5 most important features:\")\n",
    "            print(feature_importance.head())\n",
    "\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Random Forest training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def _calculate_feature_importance(self):\n",
    "        \"\"\"Calculate feature importance using correlation and variance\"\"\"\n",
    "        numeric_features = self.train_data.select_dtypes(include=[np.number]).columns\n",
    "        numeric_features = [\n",
    "            col\n",
    "            for col in numeric_features\n",
    "            if col not in [\"Store\", \"Dept\", \"Weekly_Sales\"]\n",
    "        ]\n",
    "\n",
    "        # Calculate correlation with target\n",
    "        correlations = {}\n",
    "        for feature in numeric_features:\n",
    "            if feature in self.train_data.columns:\n",
    "                corr = abs(\n",
    "                    self.train_data[feature].corr(self.train_data[\"Weekly_Sales\"])\n",
    "                )\n",
    "                correlations[feature] = corr if not np.isnan(corr) else 0\n",
    "\n",
    "        return pd.Series(correlations).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from src.data_loader import WalmartDataLoader\n",
    "    from src.data_processing import WalmartComprehensiveEDA\n",
    "    from src.feature_engineering import WalmartFeatureEngineering\n",
    "\n",
    "    data_loader = WalmartDataLoader()\n",
    "    data_loader.load_data()\n",
    "    train_data = data_loader.train_data\n",
    "    test_data = data_loader.test_data\n",
    "    features_data = data_loader.features_data\n",
    "    stores_data = data_loader.stores_data\n",
    "\n",
    "    eda = WalmartComprehensiveEDA(train_data, test_data, features_data, stores_data)\n",
    "    merged_data = eda.merge_datasets()\n",
    "\n",
    "    feature_eng = WalmartFeatureEngineering(merged_data)\n",
    "    processed_data = feature_eng.create_walmart_features()\n",
    "    processed_data = feature_eng.handle_missing_values()\n",
    "    print(\"Feature Engineering class ready!\")\n",
    "\n",
    "    forecasting_models = WalmartForecastingModels(processed_data)\n",
    "    train_data, val_data = forecasting_models.prepare_walmart_data()\n",
    "\n",
    "    # models\n",
    "    prophet_model, prophet_pred = forecasting_models.prophet_walmart_model()\n",
    "    lstm_model, lstm_pred = forecasting_models.lstm_walmart_model()\n",
    "    rf_model, rf_pred = forecasting_models.random_forest_walmart_model()\n",
    "    trans_model, trans_pred = forecasting_models.transformer_walmart_model()\n",
    "\n",
    "    print(\"Forecasting Models class ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banking_ts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
