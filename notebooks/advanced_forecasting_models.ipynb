{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e81499",
   "metadata": {},
   "source": [
    "# Advanced Walmart Forecasting Models Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `AdvancedWalmartForecastingModels` class implements a comprehensive ensemble of state-of-the-art time series forecasting models specifically designed for the Walmart Sales Forecasting Challenge. This system combines deep learning, statistical modeling, and probabilistic approaches to predict weekly sales across multiple stores and departments.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "### Data Pipeline\n",
    "- **Input**: Historical sales data with features (temperature, fuel price, CPI, unemployment, holidays, markdowns)\n",
    "- **Processing**: Time-based train/validation split, feature scaling, sequence creation\n",
    "- **Output**: Multi-step ahead forecasts with uncertainty quantification\n",
    "\n",
    "### Model Portfolio\n",
    "The system implements five distinct forecasting approaches:\n",
    "\n",
    "1. **Temporal Fusion Transformer (TFT) - Advanced**\n",
    "2. **Ensemble Deep Learning Model**\n",
    "3. **Neural ODE Model**\n",
    "4. **State Space Model (SARIMAX)**\n",
    "5. **Gaussian Process Model**\n",
    "\n",
    "---\n",
    "\n",
    "## Model Descriptions\n",
    "\n",
    "### 1. Temporal Fusion Transformer (TFT) - Advanced\n",
    "\n",
    "#### What it is\n",
    "A sophisticated attention-based neural network that combines the best of transformer architectures with time series-specific components for interpretable multi-horizon forecasting.\n",
    "\n",
    "#### How it's built\n",
    "```python\n",
    "# Key Components:\n",
    "- Variable Selection Network (VSN): Learns feature importance dynamically\n",
    "- Gated Residual Networks (GRN): Provides non-linear processing with skip connections\n",
    "- Multi-Head Attention: Captures temporal dependencies\n",
    "- Encoder-Decoder Architecture: Processes historical context and generates forecasts\n",
    "```\n",
    "\n",
    "#### Architecture Flow\n",
    "1. **Variable Selection**: VSN identifies the most relevant features for each time step\n",
    "2. **Feature Processing**: GRNs process selected features with gating mechanisms\n",
    "3. **Temporal Encoding**: LSTM encoder processes historical sequences\n",
    "4. **Attention Mechanism**: Multi-head attention captures complex temporal patterns\n",
    "5. **Prediction**: Global pooling + dense layers generate final forecasts\n",
    "\n",
    "#### Why it works for Walmart\n",
    "- **Variable Selection**: Automatically identifies which features (temperature, holidays, etc.) matter most for each prediction\n",
    "- **Attention Mechanisms**: Can focus on relevant historical periods (e.g., same holiday last year)\n",
    "- **Multi-horizon**: Naturally handles forecasting multiple weeks ahead\n",
    "- **Interpretability**: Provides attention weights showing which time periods and features drive predictions\n",
    "\n",
    "#### Strengths\n",
    "- State-of-the-art performance on time series benchmarks\n",
    "- Built-in interpretability through attention weights and variable selection\n",
    "- Handles multiple time series simultaneously\n",
    "- Robust to missing data and irregular patterns\n",
    "\n",
    "#### Weaknesses\n",
    "- Computationally expensive (requires significant GPU resources)\n",
    "- Many hyperparameters to tune\n",
    "- Requires substantial training data\n",
    "- Can overfit on small datasets\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Ensemble Deep Learning Model\n",
    "\n",
    "#### What it is\n",
    "A hybrid neural network that combines four different deep learning architectures in parallel to capture diverse patterns in the data.\n",
    "\n",
    "#### How it's built\n",
    "```python\n",
    "# Four Parallel Branches:\n",
    "1. LSTM Branch: Sequential pattern learning\n",
    "2. GRU Branch: Alternative recurrent processing\n",
    "3. CNN Branch: Local pattern detection\n",
    "4. Attention Branch: Global dependency modeling\n",
    "\n",
    "# Final Architecture:\n",
    "Combined Features → Dense Layers → Batch Normalization → Prediction\n",
    "```\n",
    "\n",
    "#### Architecture Details\n",
    "- **LSTM Branch**: Two-layer LSTM (64→32 units) for sequence modeling\n",
    "- **GRU Branch**: Two-layer GRU (64→32 units) as LSTM alternative\n",
    "- **CNN Branch**: 1D convolutions for local pattern detection\n",
    "- **Attention Branch**: Multi-head self-attention for global patterns\n",
    "- **Fusion**: Concatenate all branches → 64→32→1 dense layers\n",
    "\n",
    "#### Why it works for Walmart\n",
    "- **Diverse Pattern Capture**: Each branch specializes in different temporal patterns\n",
    "- **Robustness**: Ensemble approach reduces overfitting and improves generalization\n",
    "- **Complementary Strengths**: LSTM for long sequences, CNN for local patterns, attention for global dependencies\n",
    "- **Automatic Feature Learning**: No manual feature engineering required\n",
    "\n",
    "#### Strengths\n",
    "- Robust performance across different data patterns\n",
    "- Combines strengths of multiple architectures\n",
    "- Good generalization due to ensemble effect\n",
    "- Automatic feature learning\n",
    "\n",
    "#### Weaknesses\n",
    "- High computational complexity\n",
    "- Many parameters to train\n",
    "- Requires careful tuning of each branch\n",
    "- Can be unstable during training\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Neural ODE Model\n",
    "\n",
    "#### What it is\n",
    "A neural network that models the continuous-time dynamics of the system using Ordinary Differential Equations, treating time series prediction as solving a continuous dynamical system.\n",
    "\n",
    "#### How it's built\n",
    "```python\n",
    "# Core Concept: dx/dt = f(x, t)\n",
    "# Implementation:\n",
    "1. LSTM for initial temporal processing\n",
    "2. Multiple ODE residual blocks simulating continuous dynamics\n",
    "3. Euler integration: x_{t+1} = x_t + step_size * f(x_t, t)\n",
    "4. GRU for final temporal aggregation\n",
    "```\n",
    "\n",
    "#### Mathematical Foundation\n",
    "- **ODE Formulation**: Models the rate of change of sales as a function of current state and time\n",
    "- **Residual Blocks**: Each block represents one integration step\n",
    "- **Continuous Dynamics**: Captures smooth transitions between time points\n",
    "- **Regularization**: L1/L2 regularization prevents unstable dynamics\n",
    "\n",
    "#### Why it works for Walmart\n",
    "- **Smooth Transitions**: Sales changes are often gradual, matching ODE assumptions\n",
    "- **Physical Intuition**: Models sales as a continuous process influenced by economic factors\n",
    "- **Irregular Sampling**: Can handle missing time points naturally\n",
    "- **Parameter Efficiency**: Fewer parameters than traditional RNNs for similar capacity\n",
    "\n",
    "#### Strengths\n",
    "- Theoretically grounded in continuous dynamics\n",
    "- Memory efficient\n",
    "- Handles irregular time series naturally\n",
    "- Can model complex system dynamics\n",
    "\n",
    "#### Weaknesses\n",
    "- Difficult to interpret\n",
    "- Sensitive to integration step size\n",
    "- Can be unstable during training\n",
    "- Requires careful regularization\n",
    "\n",
    "---\n",
    "\n",
    "### 4. State Space Model (SARIMAX)\n",
    "\n",
    "#### What it is\n",
    "A classical econometric model that decomposes time series into trend, seasonal, and error components while incorporating external variables (temperature, fuel prices, etc.).\n",
    "\n",
    "#### How it's built\n",
    "```python\n",
    "# SARIMAX(p,d,q)(P,D,Q,s) Structure:\n",
    "- p, d, q: Non-seasonal autoregressive, differencing, moving average orders\n",
    "- P, D, Q, s: Seasonal components with period s\n",
    "- External variables: Temperature, fuel price, CPI, unemployment, holidays\n",
    "```\n",
    "\n",
    "#### Mathematical Components\n",
    "- **Autoregressive (AR)**: Current value depends on previous values\n",
    "- **Integrated (I)**: Handles non-stationary series through differencing\n",
    "- **Moving Average (MA)**: Models error terms from previous periods\n",
    "- **Seasonal**: Captures weekly/yearly patterns\n",
    "- **Exogenous**: Incorporates external economic factors\n",
    "\n",
    "#### Why it works for Walmart\n",
    "- **Economic Interpretability**: Clear relationship between economic factors and sales\n",
    "- **Seasonal Patterns**: Explicitly models weekly/yearly seasonality\n",
    "- **External Factors**: Directly incorporates weather, economic conditions\n",
    "- **Proven Track Record**: Extensively used in retail forecasting\n",
    "\n",
    "#### Strengths\n",
    "- Highly interpretable coefficients\n",
    "- Well-established statistical theory\n",
    "- Handles seasonality explicitly\n",
    "- Uncertainty quantification through confidence intervals\n",
    "- Fast training and prediction\n",
    "\n",
    "#### Weaknesses\n",
    "- Assumes linear relationships\n",
    "- Requires stationary data\n",
    "- Limited ability to capture complex non-linear patterns\n",
    "- Sensitive to outliers\n",
    "- Manual order selection required\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Gaussian Process Model\n",
    "\n",
    "#### What it is\n",
    "A non-parametric Bayesian approach that models the distribution over functions, providing both predictions and uncertainty estimates.\n",
    "\n",
    "#### How it's built\n",
    "```python\n",
    "# Key Components:\n",
    "- Kernel Function: Matérn kernel + White noise kernel\n",
    "- Prior: Gaussian process prior over functions\n",
    "- Posterior: Analytical posterior given observations\n",
    "- Prediction: Mean and variance at new points\n",
    "```\n",
    "\n",
    "#### Mathematical Foundation\n",
    "- **Kernel**: Defines similarity between time points\n",
    "- **Matérn Kernel**: Flexible kernel for modeling smooth functions\n",
    "- **White Noise**: Models observation noise\n",
    "- **Bayesian Inference**: Updates beliefs based on observed data\n",
    "\n",
    "#### Why it works for Walmart\n",
    "- **Uncertainty Quantification**: Provides confidence intervals for business decisions\n",
    "- **Non-parametric**: No assumptions about functional form\n",
    "- **Flexible**: Can model complex non-linear relationships\n",
    "- **Small Data**: Works well with limited training data\n",
    "\n",
    "#### Strengths\n",
    "- Provides uncertainty estimates\n",
    "- No assumptions about functional form\n",
    "- Works well with small datasets\n",
    "- Principled Bayesian approach\n",
    "- Can incorporate prior knowledge through kernel design\n",
    "\n",
    "#### Weaknesses\n",
    "- Computationally expensive (O(n³) scaling)\n",
    "- Limited to relatively small datasets\n",
    "- Kernel selection is crucial\n",
    "- Poor extrapolation beyond training range\n",
    "- Sensitive to hyperparameter choices\n",
    "\n",
    "---\n",
    "\n",
    "## Model Comparison\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "| Model | Complexity | Interpretability | Uncertainty | Scalability | Training Speed |\n",
    "|-------|------------|------------------|-------------|-------------|----------------|\n",
    "| TFT Advanced | Very High | Medium | No | High | Slow |\n",
    "| Ensemble DL | High | Low | No | High | Slow |\n",
    "| Neural ODE | Medium | Low | No | Medium | Medium |\n",
    "| SARIMAX | Low | Very High | Yes | Medium | Fast |\n",
    "| Gaussian Process | Medium | Medium | Yes | Low | Medium |\n",
    "\n",
    "### Use Case Recommendations\n",
    "\n",
    "#### When to use TFT Advanced\n",
    "- **Best for**: High-stakes forecasting where accuracy is paramount\n",
    "- **Requirements**: Large datasets, GPU resources, interpretability needs\n",
    "- **Example**: Strategic planning, inventory optimization\n",
    "\n",
    "#### When to use Ensemble Deep Learning\n",
    "- **Best for**: Robust performance across diverse data patterns\n",
    "- **Requirements**: Sufficient training data, computational resources\n",
    "- **Example**: Automated forecasting systems, multiple product lines\n",
    "\n",
    "#### When to use Neural ODE\n",
    "- **Best for**: Systems with smooth, continuous dynamics\n",
    "- **Requirements**: Understanding of differential equations, irregular time series\n",
    "- **Example**: Economic modeling, supply chain dynamics\n",
    "\n",
    "#### When to use SARIMAX\n",
    "- **Best for**: Baseline models, interpretable results, limited data\n",
    "- **Requirements**: Domain expertise for model specification\n",
    "- **Example**: Financial reporting, regulatory compliance\n",
    "\n",
    "#### When to use Gaussian Process\n",
    "- **Best for**: Uncertainty-critical decisions, small datasets\n",
    "- **Requirements**: Computational limits acceptable, need for confidence intervals\n",
    "- **Example**: Risk assessment, A/B testing, new product launches\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Data Preprocessing\n",
    "```python\n",
    "# Common preprocessing steps:\n",
    "1. Time-based train/validation split\n",
    "2. Feature scaling using StandardScaler\n",
    "3. Sequence creation for neural models\n",
    "4. Missing value handling\n",
    "5. Aggregation by store/date for efficiency\n",
    "```\n",
    "\n",
    "### Feature Engineering\n",
    "- **Temporal Features**: Date-based features, lag variables\n",
    "- **Economic Indicators**: Temperature, fuel price, CPI, unemployment\n",
    "- **Business Features**: Store size, holiday indicators, markdowns\n",
    "- **Engineered Features**: Holiday weights, total markdowns\n",
    "\n",
    "### Validation Strategy\n",
    "- **Time-based split**: Last 8 weeks for validation\n",
    "- **Walk-forward validation**: Simulates real-world deployment\n",
    "- **Metrics**: WMAE (Weighted Mean Absolute Error) matching competition\n",
    "\n",
    "### Training Configuration\n",
    "- **Early Stopping**: Prevents overfitting\n",
    "- **Learning Rate Scheduling**: Adaptive learning rates\n",
    "- **Regularization**: Dropout, L1/L2 regularization\n",
    "- **Batch Processing**: Efficient memory usage\n",
    "\n",
    "---\n",
    "\n",
    "## Business Impact\n",
    "\n",
    "### Forecasting Accuracy\n",
    "- **TFT**: Highest accuracy, suitable for strategic decisions\n",
    "- **Ensemble**: Consistent performance across different scenarios\n",
    "- **Neural ODE**: Good for smooth demand patterns\n",
    "- **SARIMAX**: Reliable baseline with clear interpretation\n",
    "- **Gaussian Process**: Best when uncertainty quantification is critical\n",
    "\n",
    "### Computational Requirements\n",
    "- **Training Time**: SARIMAX < GP < Neural ODE < Ensemble < TFT\n",
    "- **Memory Usage**: SARIMAX < GP < Neural ODE < TFT < Ensemble\n",
    "- **Inference Speed**: All models provide fast inference for business use\n",
    "\n",
    "### Operational Considerations\n",
    "- **Model Maintenance**: Simpler models require less maintenance\n",
    "- **Interpretability**: Critical for stakeholder buy-in and regulatory compliance\n",
    "- **Uncertainty**: Essential for inventory planning and risk management\n",
    "- **Scalability**: Important for enterprise deployment across thousands of stores\n",
    "\n",
    "---\n",
    "\n",
    "## Future Enhancements\n",
    "\n",
    "### Model Improvements\n",
    "1. **Hierarchical Forecasting**: Coherent forecasts across store/department hierarchy\n",
    "2. **Causal Inference**: Incorporating causal relationships between variables\n",
    "3. **Transfer Learning**: Leveraging knowledge across similar stores\n",
    "4. **Online Learning**: Continuous model updates with new data\n",
    "\n",
    "### Technical Enhancements\n",
    "1. **Model Ensembling**: Combining predictions from multiple models\n",
    "2. **Hyperparameter Optimization**: Automated tuning using Bayesian optimization\n",
    "3. **Distributed Training**: Scaling to larger datasets and model complexity\n",
    "4. **Model Monitoring**: Detecting model degradation and concept drift\n",
    "\n",
    "### Business Applications\n",
    "1. **Inventory Optimization**: Using forecasts for stock level decisions\n",
    "2. **Pricing Strategy**: Dynamic pricing based on demand forecasts\n",
    "3. **Resource Allocation**: Staff scheduling and supply chain planning\n",
    "4. **Risk Management**: Scenario planning and stress testing\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This advanced forecasting system provides a comprehensive toolkit for retail demand prediction. Each model offers unique strengths:\n",
    "\n",
    "- **TFT** for maximum accuracy and interpretability\n",
    "- **Ensemble** for robustness across scenarios\n",
    "- **Neural ODE** for modeling continuous dynamics\n",
    "- **SARIMAX** for statistical rigor and interpretability\n",
    "- **Gaussian Process** for uncertainty quantification\n",
    "\n",
    "The choice of model depends on specific business requirements, computational constraints, and the level of interpretability needed. In practice, combining multiple models often provides the best results, leveraging the strengths of each approach while mitigating individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7277c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded successfully!\n",
      "============================================================\n",
      "MERGING DATASETS\n",
      "============================================================\n",
      "Merged training data shape: (421570, 16)\n",
      "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
      "Number of stores: 45\n",
      "Number of departments: 81\n",
      "=== WALMART-SPECIFIC FEATURE ENGINEERING ===\n",
      "Feature engineering completed. New shape: (421570, 67)\n",
      "Added 62 new features\n",
      "=== HANDLING MISSING VALUES ===\n",
      "Missing values handled. Remaining NaN: 0\n",
      "Feature Engineering class ready!\n",
      "=== PREPARING DATA FOR MODELING ===\n",
      "Data split completed:\n",
      "  - Training data: (397841, 67)\n",
      "  - Validation data: (23729, 67)\n",
      "  - Feature columns: ['IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'Holiday_Weight', 'Pre_Holiday', 'Post_Holiday', 'Year', 'Month', 'Week', 'Quarter', 'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', 'Sales_lag_1', 'Sales_lag_2', 'Sales_lag_4', 'Sales_lag_8', 'Sales_lag_12', 'Sales_lag_52', 'Sales_rolling_mean_4', 'Sales_rolling_std_4', 'Sales_rolling_max_4', 'Sales_rolling_min_4', 'Sales_rolling_mean_8', 'Sales_rolling_std_8', 'Sales_rolling_max_8', 'Sales_rolling_min_8', 'Sales_rolling_mean_12', 'Sales_rolling_std_12', 'Sales_rolling_max_12', 'Sales_rolling_min_12', 'Sales_rolling_mean_24', 'Sales_rolling_std_24', 'Sales_rolling_max_24', 'Sales_rolling_min_24', 'Sales_rolling_mean_52', 'Sales_rolling_std_52', 'Sales_rolling_max_52', 'Sales_rolling_min_52', 'Total_MarkDown', 'MarkDown1_Active', 'MarkDown2_Active', 'MarkDown3_Active', 'MarkDown4_Active', 'MarkDown5_Active', 'MarkDown_Intensity', 'Store_Type_Avg', 'Dept_Avg', 'Store_Dept_Avg', 'Unemployment_Temperature', 'CPI_Fuel_Interaction', 'Economic_Stress', 'Sales_Trend']\n",
      "  - Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
      "  - Split date: 2012-09-07 00:00:00\n",
      "=== TRAINING ADVANCED TEMPORAL FUSION TRANSFORMER ===\n",
      "Advanced TFT model trained in 25.38 seconds\n",
      "=== TRAINING ENSEMBLE DEEP LEARNING MODEL ===\n",
      "Model input shape: (None, 5, 6)\n",
      "Sequence length: 5, Features: 6\n",
      "Ensemble Deep Learning model trained in 32.57 seconds\n",
      "=== TRAINING NEURAL ODE MODEL (CORRECTED) ===\n",
      "Training sequences: 5850\n",
      "Validation sequences: 135\n",
      "Model input shape: (None, 5, 6)\n",
      "Sequence length: 5, Features: 6\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3a6ed1080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3a6ed1080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Neural ODE model trained in 39.73 seconds\n",
      "Predictions shape: (135,), Actual shape: (135,)\n",
      "=== TRAINING STATE SPACE MODEL (SARIMAX) - COMPLETE FIXED ===\n",
      "Training data shape before processing: (135, 7)\n",
      "Date range: 2010-02-05 00:00:00 to 2012-08-31 00:00:00\n",
      "Using resample instead of asfreq...\n",
      "Shape after resample: (135, 6)\n",
      "NaN values after resample: 0\n",
      "Shape after filling: (135, 6)\n",
      "Converting data types safely...\n",
      "Converting Weekly_Sales: float64 -> float64\n",
      "Converting Temperature: float64 -> float64\n",
      "Converting Fuel_Price: float64 -> float64\n",
      "Converting CPI: float64 -> float64\n",
      "Converting Unemployment: float64 -> float64\n",
      "Converting IsHoliday: bool -> float64\n",
      "Final training shape: (135, 6)\n",
      "Final training NaN count: 0\n",
      "Training data types: Weekly_Sales    float64\n",
      "Temperature     float64\n",
      "Fuel_Price      float64\n",
      "CPI             float64\n",
      "Unemployment    float64\n",
      "IsHoliday       float64\n",
      "dtype: object\n",
      "Fitting SARIMAX model...\n",
      "Validation data shape before processing: (8, 7)\n",
      "Using resample for validation data...\n",
      "Validation shape after resample: (8, 6)\n",
      "Validation NaN values after resample: 0\n",
      "Converting validation data types safely...\n",
      "Converting validation Weekly_Sales: float64 -> float64\n",
      "Converting validation Temperature: float64 -> float64\n",
      "Converting validation Fuel_Price: float64 -> float64\n",
      "Converting validation CPI: float64 -> float64\n",
      "Converting validation Unemployment: float64 -> float64\n",
      "Converting validation IsHoliday: bool -> float64\n",
      "Final validation shape: (8, 6)\n",
      "Final validation NaN count: 0\n",
      "Forecasting 8 steps...\n",
      "SARIMAX model trained successfully in 1.77 seconds\n",
      "Forecast shape: (8,)\n",
      "Actual shape: (8,)\n",
      "=== TRAINING GAUSSIAN PROCESS MODEL ===\n",
      "Gaussian Process model trained in 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Core libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GRU,\n",
    "    Input,\n",
    "    MultiHeadAttention,\n",
    "    LayerNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Concatenate,\n",
    "    BatchNormalization,\n",
    "    Add,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Statistical models\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Advanced time series\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyTorch not available. Install with: pip install torch\")\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "\n",
    "class AdvancedWalmartForecastingModels:\n",
    "    \"\"\"Advanced forecasting models for Walmart competition including hierarchical and causal approaches\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.hierarchical_structure = None\n",
    "        self.causal_graph = None\n",
    "        self.feature_columns = []\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "\n",
    "    def prepare_data(self, validation_weeks=8):\n",
    "        \"\"\"Prepare data for time series modeling with train/validation split\"\"\"\n",
    "        print(\"=== PREPARING DATA FOR MODELING ===\")\n",
    "\n",
    "        # Clean and sort data\n",
    "        self.data_clean = self.data.dropna(subset=[\"Weekly_Sales\"])\n",
    "        self.data_clean = self.data_clean.sort_values([\"Store\", \"Dept\", \"Date\"])\n",
    "\n",
    "        # Time-based split\n",
    "        unique_dates = sorted(self.data_clean[\"Date\"].unique())\n",
    "        split_date = unique_dates[-validation_weeks]\n",
    "\n",
    "        self.train_data = self.data_clean[self.data_clean[\"Date\"] < split_date].copy()\n",
    "        self.val_data = self.data_clean[self.data_clean[\"Date\"] >= split_date].copy()\n",
    "\n",
    "        # Select features (excluding target and identifier columns)\n",
    "        exclude_cols = [\"Weekly_Sales\", \"Store\", \"Dept\", \"Date\"]\n",
    "        self.feature_columns = [\n",
    "            col\n",
    "            for col in self.data_clean.columns\n",
    "            if col not in exclude_cols and not col.endswith(\"_scaled\")\n",
    "        ]\n",
    "\n",
    "        print(f\"Data split completed:\")\n",
    "        print(f\"  - Training data: {self.train_data.shape}\")\n",
    "        print(f\"  - Validation data: {self.val_data.shape}\")\n",
    "        print(f\"  - Feature columns: {self.feature_columns}\")\n",
    "        print(\n",
    "            f\"  - Date range: {self.data_clean['Date'].min()} to {self.data_clean['Date'].max()}\"\n",
    "        )\n",
    "        print(f\"  - Split date: {split_date}\")\n",
    "\n",
    "        return self.train_data, self.val_data\n",
    "\n",
    "    def temporal_fusion_transformer_advanced(self, sequence_length=5, epochs=50):\n",
    "        \"\"\"Advanced Temporal Fusion Transformer with full architecture\"\"\"\n",
    "        print(\"=== TRAINING ADVANCED TEMPORAL FUSION TRANSFORMER ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Variable Selection Network\n",
    "            class VariableSelectionNetwork(tf.keras.layers.Layer):\n",
    "                def __init__(self, num_features, hidden_size, dropout_rate=0.1):\n",
    "                    super(VariableSelectionNetwork, self).__init__()\n",
    "                    self.num_features = num_features\n",
    "                    self.hidden_size = hidden_size\n",
    "\n",
    "                    self.linear1 = Dense(hidden_size, activation=\"relu\")\n",
    "                    self.linear2 = Dense(num_features, activation=\"softmax\")\n",
    "                    self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "                def call(self, inputs, training=None):\n",
    "                    # inputs: [batch_size, time_steps, num_features]\n",
    "                    batch_size = tf.shape(inputs)[0]\n",
    "                    time_steps = tf.shape(inputs)[1]\n",
    "\n",
    "                    # Flatten time dimension for processing\n",
    "                    flattened = tf.reshape(inputs, [-1, self.num_features])\n",
    "\n",
    "                    # Variable selection\n",
    "                    x = self.linear1(flattened)\n",
    "                    x = self.dropout(x, training=training)\n",
    "                    weights = self.linear2(x)\n",
    "\n",
    "                    # Reshape back\n",
    "                    weights = tf.reshape(\n",
    "                        weights, [batch_size, time_steps, self.num_features]\n",
    "                    )\n",
    "\n",
    "                    # Apply variable selection\n",
    "                    selected = inputs * weights\n",
    "                    return selected, weights\n",
    "\n",
    "            # Gated Residual Network\n",
    "            class GatedResidualNetwork(tf.keras.layers.Layer):\n",
    "                def __init__(self, hidden_size, dropout_rate=0.1):\n",
    "                    super(GatedResidualNetwork, self).__init__()\n",
    "                    self.hidden_size = hidden_size\n",
    "\n",
    "                    self.linear1 = Dense(hidden_size, activation=\"relu\")\n",
    "                    self.linear2 = Dense(hidden_size)\n",
    "                    self.gate = Dense(hidden_size, activation=\"sigmoid\")\n",
    "                    self.dropout = Dropout(dropout_rate)\n",
    "                    self.layer_norm = LayerNormalization()\n",
    "\n",
    "                def call(self, inputs, training=None):\n",
    "                    x = self.linear1(inputs)\n",
    "                    x = self.dropout(x, training=training)\n",
    "                    x = self.linear2(x)\n",
    "\n",
    "                    gate = self.gate(inputs)\n",
    "\n",
    "                    # Gated residual connection\n",
    "                    output = gate * x + (1 - gate) * inputs\n",
    "                    output = self.layer_norm(output)\n",
    "\n",
    "                    return output\n",
    "\n",
    "            # Prepare data for TFT\n",
    "            agg_train = (\n",
    "                self.train_data.groupby([\"Store\", \"Date\"])\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"CPI\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                        \"Total_MarkDown\": \"sum\",\n",
    "                        \"Holiday_Weight\": \"max\",\n",
    "                        \"Size\": \"first\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            agg_val = (\n",
    "                self.val_data.groupby([\"Store\", \"Date\"])\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"CPI\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                        \"Total_MarkDown\": \"sum\",\n",
    "                        \"Holiday_Weight\": \"max\",\n",
    "                        \"Size\": \"first\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            # Features\n",
    "            continuous_features = [\n",
    "                \"Temperature\",\n",
    "                \"Fuel_Price\",\n",
    "                \"CPI\",\n",
    "                \"Unemployment\",\n",
    "                \"Total_MarkDown\",\n",
    "                \"Size\",\n",
    "            ]\n",
    "            categorical_features = [\"IsHoliday\"]\n",
    "\n",
    "            # Create sequences\n",
    "            def create_tft_sequences(data, seq_len):\n",
    "                X_cont, X_cat, y, weights = [], [], [], []\n",
    "\n",
    "                for store in data[\"Store\"].unique():\n",
    "                    store_data = data[data[\"Store\"] == store].sort_values(\"Date\")\n",
    "\n",
    "                    if len(store_data) < seq_len + 1:\n",
    "                        continue\n",
    "\n",
    "                    for i in range(seq_len, len(store_data)):\n",
    "                        X_cont.append(\n",
    "                            store_data[continuous_features].iloc[i - seq_len : i].values\n",
    "                        )\n",
    "                        X_cat.append(\n",
    "                            store_data[categorical_features]\n",
    "                            .iloc[i - seq_len : i]\n",
    "                            .values\n",
    "                        )\n",
    "                        y.append(store_data[\"Weekly_Sales\"].iloc[i])\n",
    "                        weights.append(store_data[\"Holiday_Weight\"].iloc[i])\n",
    "\n",
    "                return np.array(X_cont), np.array(X_cat), np.array(y), np.array(weights)\n",
    "\n",
    "            X_cont_train, X_cat_train, y_train, train_weights = create_tft_sequences(\n",
    "                agg_train, sequence_length\n",
    "            )\n",
    "            X_cont_val, X_cat_val, y_val, val_weights = create_tft_sequences(\n",
    "                agg_val, sequence_length\n",
    "            )\n",
    "\n",
    "            if len(X_cont_train) == 0 or len(X_cont_val) == 0:\n",
    "                print(\"Insufficient data for TFT\")\n",
    "                return None, None\n",
    "\n",
    "            # Scale continuous features\n",
    "            scaler_cont = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_cont_train_scaled = scaler_cont.fit_transform(\n",
    "                X_cont_train.reshape(-1, X_cont_train.shape[-1])\n",
    "            ).reshape(X_cont_train.shape)\n",
    "\n",
    "            X_cont_val_scaled = scaler_cont.transform(\n",
    "                X_cont_val.reshape(-1, X_cont_val.shape[-1])\n",
    "            ).reshape(X_cont_val.shape)\n",
    "\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Build TFT model\n",
    "            hidden_size = 64\n",
    "            num_cont_features = len(continuous_features)\n",
    "            num_cat_features = len(categorical_features)\n",
    "\n",
    "            # Inputs\n",
    "            cont_inputs = Input(\n",
    "                shape=(sequence_length, num_cont_features), name=\"continuous\"\n",
    "            )\n",
    "            cat_inputs = Input(\n",
    "                shape=(sequence_length, num_cat_features), name=\"categorical\"\n",
    "            )\n",
    "\n",
    "            # Embed categorical features\n",
    "            cat_embedded = Dense(8, activation=\"relu\")(cat_inputs)\n",
    "\n",
    "            # Combine continuous and categorical\n",
    "            combined = Concatenate(axis=-1)([cont_inputs, cat_embedded])\n",
    "\n",
    "            # Variable Selection Network\n",
    "            vsn = VariableSelectionNetwork(num_cont_features + 8, hidden_size)\n",
    "            selected_features, variable_weights = vsn(combined)\n",
    "\n",
    "            # Encoder-Decoder with attention\n",
    "            # Encoder\n",
    "            encoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "            encoder_outputs, state_h, state_c = encoder_lstm(selected_features)\n",
    "\n",
    "            # Self-attention\n",
    "            attention = MultiHeadAttention(num_heads=4, key_dim=hidden_size // 4)\n",
    "            attention_output = attention(encoder_outputs, encoder_outputs)\n",
    "\n",
    "            # Gated Residual Network\n",
    "            grn = GatedResidualNetwork(hidden_size)\n",
    "            grn_output = grn(attention_output)\n",
    "\n",
    "            # Global pooling and output\n",
    "            pooled = GlobalAveragePooling1D()(grn_output)\n",
    "\n",
    "            # Final prediction layers\n",
    "            dense1 = Dense(hidden_size, activation=\"relu\")(pooled)\n",
    "            dropout1 = Dropout(0.3)(dense1)\n",
    "            dense2 = Dense(hidden_size // 2, activation=\"relu\")(dropout1)\n",
    "            dropout2 = Dropout(0.2)(dense2)\n",
    "            output = Dense(1)(dropout2)\n",
    "\n",
    "            # Create model\n",
    "            model = Model(inputs=[cont_inputs, cat_inputs], outputs=output)\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"]\n",
    "            )\n",
    "\n",
    "            # Train model\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                [X_cont_train_scaled, X_cat_train],\n",
    "                y_train_scaled,\n",
    "                validation_data=(\n",
    "                    [X_cont_val_scaled, X_cat_val],\n",
    "                    scaler_y.transform(y_val.reshape(-1, 1)).flatten(),\n",
    "                ),\n",
    "                epochs=epochs,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred_scaled = model.predict([X_cont_val_scaled, X_cat_val], verbose=0)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            self.models[\"TFT_Advanced\"] = model\n",
    "            self.results[\"TFT_Advanced\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val,\n",
    "                \"weights\": val_weights,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Neural Network\",\n",
    "                \"history\": history,\n",
    "                \"variable_weights\": None,  # Would extract from model\n",
    "                \"attention_weights\": None,  # Would extract from model\n",
    "            }\n",
    "\n",
    "            print(f\"Advanced TFT model trained in {training_time:.2f} seconds\")\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Advanced TFT training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def ensemble_deep_learning_model(self, sequence_length=5, epochs=40):\n",
    "        \"\"\"Ensemble of multiple deep learning architectures - FIXED VERSION\"\"\"\n",
    "        print(\"=== TRAINING ENSEMBLE DEEP LEARNING MODEL ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Prepare sequences\n",
    "            features = [\n",
    "                \"Temperature\",\n",
    "                \"Fuel_Price\",\n",
    "                \"CPI\",\n",
    "                \"Unemployment\",\n",
    "                \"IsHoliday\",\n",
    "                \"Total_MarkDown\",\n",
    "            ]\n",
    "\n",
    "            def create_sequences(data, seq_len, features):\n",
    "                X, y, weights = [], [], []\n",
    "\n",
    "                # Aggregate by store and date for efficiency\n",
    "                agg_data = (\n",
    "                    data.groupby([\"Store\", \"Date\"])\n",
    "                    .agg(\n",
    "                        {\n",
    "                            \"Weekly_Sales\": \"sum\",\n",
    "                            **{\n",
    "                                feat: \"mean\" if feat != \"IsHoliday\" else \"max\"\n",
    "                                for feat in features\n",
    "                            },\n",
    "                            \"Holiday_Weight\": \"max\",\n",
    "                        }\n",
    "                    )\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                for store in agg_data[\"Store\"].unique():\n",
    "                    store_data = agg_data[agg_data[\"Store\"] == store].sort_values(\n",
    "                        \"Date\"\n",
    "                    )\n",
    "\n",
    "                    if len(store_data) < seq_len + 1:\n",
    "                        continue\n",
    "\n",
    "                    for i in range(seq_len, len(store_data)):\n",
    "                        X.append(store_data[features].iloc[i - seq_len : i].values)\n",
    "                        y.append(store_data[\"Weekly_Sales\"].iloc[i])\n",
    "                        weights.append(store_data[\"Holiday_Weight\"].iloc[i])\n",
    "\n",
    "                return np.array(X), np.array(y), np.array(weights)\n",
    "\n",
    "            X_train, y_train, train_weights = create_sequences(\n",
    "                self.train_data, sequence_length, features\n",
    "            )\n",
    "            X_val, y_val, val_weights = create_sequences(\n",
    "                self.val_data, sequence_length, features\n",
    "            )\n",
    "\n",
    "            if len(X_train) == 0 or len(X_val) == 0:\n",
    "                print(\"Insufficient data for ensemble model\")\n",
    "                return None, None\n",
    "\n",
    "            # Scale data\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(\n",
    "                X_train.reshape(-1, X_train.shape[-1])\n",
    "            ).reshape(X_train.shape)\n",
    "            X_val_scaled = scaler_X.transform(\n",
    "                X_val.reshape(-1, X_val.shape[-1])\n",
    "            ).reshape(X_val.shape)\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Define multiple architectures\n",
    "            def create_lstm_branch(inputs):\n",
    "                x = LSTM(64, return_sequences=True)(inputs)\n",
    "                x = Dropout(0.3)(x)\n",
    "                x = LSTM(32, return_sequences=False)(x)\n",
    "                x = Dropout(0.2)(x)\n",
    "                return Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "            def create_gru_branch(inputs):\n",
    "                x = GRU(64, return_sequences=True)(inputs)\n",
    "                x = Dropout(0.3)(x)\n",
    "                x = GRU(32, return_sequences=False)(x)\n",
    "                x = Dropout(0.2)(x)\n",
    "                return Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "            def create_cnn_branch(inputs):\n",
    "                # FIXED: Adjusted for short sequences\n",
    "                if sequence_length <= 3:\n",
    "                    # For very short sequences, use single conv layer without pooling\n",
    "                    x = Conv1D(\n",
    "                        filters=32, kernel_size=2, activation=\"relu\", padding=\"same\"\n",
    "                    )(inputs)\n",
    "                    x = GlobalAveragePooling1D()(x)\n",
    "                    return Dense(16, activation=\"relu\")(x)\n",
    "                else:\n",
    "                    # For longer sequences, use the original approach but with padding\n",
    "                    x = Conv1D(\n",
    "                        filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
    "                    )(inputs)\n",
    "\n",
    "                    # Only apply pooling if sequence length allows it\n",
    "                    if sequence_length > 4:\n",
    "                        x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "                    # Adjust kernel size for second conv layer based on remaining sequence length\n",
    "                    remaining_length = (\n",
    "                        sequence_length // 2 if sequence_length > 4 else sequence_length\n",
    "                    )\n",
    "                    kernel_size = min(3, remaining_length)\n",
    "\n",
    "                    if kernel_size >= 2:\n",
    "                        x = Conv1D(\n",
    "                            filters=32,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation=\"relu\",\n",
    "                            padding=\"same\",\n",
    "                        )(x)\n",
    "\n",
    "                    x = GlobalAveragePooling1D()(x)\n",
    "                    return Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "            def create_attention_branch(inputs):\n",
    "                # Ensure key_dim is reasonable for the sequence length\n",
    "                key_dim = min(32, max(8, len(features) // 2))\n",
    "                x = MultiHeadAttention(num_heads=4, key_dim=key_dim)(inputs, inputs)\n",
    "                x = LayerNormalization()(x)\n",
    "                x = GlobalAveragePooling1D()(x)\n",
    "                return Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "            # Input layer\n",
    "            inputs = Input(shape=(sequence_length, len(features)))\n",
    "\n",
    "            # Create branches\n",
    "            lstm_branch = create_lstm_branch(inputs)\n",
    "            gru_branch = create_gru_branch(inputs)\n",
    "            cnn_branch = create_cnn_branch(inputs)\n",
    "            attention_branch = create_attention_branch(inputs)\n",
    "\n",
    "            # Combine branches\n",
    "            combined = Concatenate()(\n",
    "                [lstm_branch, gru_branch, cnn_branch, attention_branch]\n",
    "            )\n",
    "\n",
    "            # Final layers\n",
    "            x = Dense(64, activation=\"relu\")(combined)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(0.4)(x)\n",
    "            x = Dense(32, activation=\"relu\")(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            outputs = Dense(1)(x)\n",
    "\n",
    "            # Create and compile model\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"]\n",
    "            )\n",
    "\n",
    "            print(f\"Model input shape: {inputs.shape}\")\n",
    "            print(f\"Sequence length: {sequence_length}, Features: {len(features)}\")\n",
    "\n",
    "            # Train model\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-6\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_scaled,\n",
    "                validation_data=(\n",
    "                    X_val_scaled,\n",
    "                    scaler_y.transform(y_val.reshape(-1, 1)).flatten(),\n",
    "                ),\n",
    "                epochs=epochs,\n",
    "                batch_size=64,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred_scaled = model.predict(X_val_scaled, verbose=0)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            self.models[\"EnsembleDeep\"] = model\n",
    "            self.results[\"EnsembleDeep\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val,\n",
    "                \"weights\": val_weights,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Neural Network\",\n",
    "                \"history\": history,\n",
    "                \"architecture\": \"LSTM+GRU+CNN+Attention\",\n",
    "            }\n",
    "\n",
    "            print(\n",
    "                f\"Ensemble Deep Learning model trained in {training_time:.2f} seconds\"\n",
    "            )\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ensemble Deep Learning training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def neural_ode_model(self, sequence_length=5, epochs=30):\n",
    "        \"\"\"Neural ODE model - CORRECTED VERSION to match TFT and Ensemble DL structure\"\"\"\n",
    "        print(\"=== TRAINING NEURAL ODE MODEL (CORRECTED) ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Use same feature set as other models for consistency\n",
    "            features = [\n",
    "                \"Temperature\",\n",
    "                \"Fuel_Price\",\n",
    "                \"CPI\",\n",
    "                \"Unemployment\",\n",
    "                \"IsHoliday\",\n",
    "                \"Total_MarkDown\",  # Added this feature like other models\n",
    "            ]\n",
    "\n",
    "            # Use the SAME sequence creation logic as ensemble model\n",
    "            def create_sequences(data, seq_len, features):\n",
    "                X, y, weights = [], [], []\n",
    "\n",
    "                # Aggregate by store and date for efficiency (SAME AS ENSEMBLE)\n",
    "                agg_data = (\n",
    "                    data.groupby([\"Store\", \"Date\"])\n",
    "                    .agg(\n",
    "                        {\n",
    "                            \"Weekly_Sales\": \"sum\",\n",
    "                            **{\n",
    "                                feat: \"mean\" if feat != \"IsHoliday\" else \"max\"\n",
    "                                for feat in features\n",
    "                            },\n",
    "                            \"Holiday_Weight\": \"max\",\n",
    "                        }\n",
    "                    )\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                for store in agg_data[\"Store\"].unique():\n",
    "                    store_data = agg_data[agg_data[\"Store\"] == store].sort_values(\n",
    "                        \"Date\"\n",
    "                    )\n",
    "\n",
    "                    if len(store_data) < seq_len + 1:\n",
    "                        continue\n",
    "\n",
    "                    for i in range(seq_len, len(store_data)):\n",
    "                        X.append(store_data[features].iloc[i - seq_len : i].values)\n",
    "                        y.append(store_data[\"Weekly_Sales\"].iloc[i])\n",
    "                        weights.append(store_data[\"Holiday_Weight\"].iloc[i])\n",
    "\n",
    "                return np.array(X), np.array(y), np.array(weights)\n",
    "\n",
    "            # Create training and validation sequences (SAME AS OTHER MODELS)\n",
    "            X_train, y_train, train_weights = create_sequences(\n",
    "                self.train_data, sequence_length, features\n",
    "            )\n",
    "            X_val, y_val, val_weights = create_sequences(\n",
    "                self.val_data, sequence_length, features\n",
    "            )\n",
    "\n",
    "            print(f\"Training sequences: {len(X_train)}\")\n",
    "            print(f\"Validation sequences: {len(X_val)}\")\n",
    "\n",
    "            if len(X_train) == 0 or len(X_val) == 0:\n",
    "                print(\"Insufficient data for Neural ODE\")\n",
    "                return None, None\n",
    "\n",
    "            # Scale data (SAME AS OTHER MODELS)\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(\n",
    "                X_train.reshape(-1, X_train.shape[-1])\n",
    "            ).reshape(X_train.shape)\n",
    "            X_val_scaled = scaler_X.transform(\n",
    "                X_val.reshape(-1, X_val.shape[-1])\n",
    "            ).reshape(X_val.shape)\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Neural ODE Architecture - IMPROVED\n",
    "            inputs = Input(shape=(sequence_length, len(features)))\n",
    "\n",
    "            # Initial processing layer\n",
    "            hidden_dim = 64\n",
    "\n",
    "            # Process temporal sequences first\n",
    "            x = LSTM(hidden_dim, return_sequences=True, return_state=False)(inputs)\n",
    "            x = LayerNormalization()(x)\n",
    "\n",
    "            # Neural ODE blocks - simulating continuous dynamics\n",
    "            def ode_residual_block(x, step_size=0.1):\n",
    "                \"\"\"\n",
    "                Simulates one step of ODE integration using residual connections\n",
    "                dx/dt = f(x, t) approximated as x_{t+1} = x_t + step_size * f(x_t, t)\n",
    "                \"\"\"\n",
    "                residual = x  # x_t\n",
    "\n",
    "                # f(x_t, t) - the derivative function\n",
    "                dx = Dense(\n",
    "                    hidden_dim,\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "                )(x)\n",
    "                dx = Dropout(0.1)(dx)  # Regularization\n",
    "                dx = Dense(\n",
    "                    hidden_dim,\n",
    "                    activation=\"tanh\",\n",
    "                    kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "                )(dx)\n",
    "\n",
    "                # Euler integration: x_{t+1} = x_t + step_size * dx/dt\n",
    "                # We'll use step_size = 1 for simplicity, but could be learnable\n",
    "                x_new = Add()([residual, dx])\n",
    "\n",
    "                # Normalize to prevent exploding gradients\n",
    "                return LayerNormalization()(x_new)\n",
    "\n",
    "            # Apply multiple ODE integration steps\n",
    "            num_ode_steps = 6  # Simulate 6 time steps of continuous dynamics\n",
    "            for step in range(num_ode_steps):\n",
    "                x = ode_residual_block(x, step_size=0.1)\n",
    "\n",
    "            # Additional processing for better temporal modeling\n",
    "            x = GRU(32, return_sequences=False)(x)  # Final temporal aggregation\n",
    "            x = Dropout(0.3)(x)\n",
    "\n",
    "            # Output layers\n",
    "            x = Dense(64, activation=\"relu\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(0.4)(x)\n",
    "            x = Dense(32, activation=\"relu\")(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            outputs = Dense(1)(x)\n",
    "\n",
    "            # Create and compile model\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"]\n",
    "            )\n",
    "\n",
    "            print(f\"Model input shape: {inputs.shape}\")\n",
    "            print(f\"Sequence length: {sequence_length}, Features: {len(features)}\")\n",
    "\n",
    "            # Train model with SAME callbacks as other models\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-6\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_scaled,\n",
    "                validation_data=(\n",
    "                    X_val_scaled,\n",
    "                    scaler_y.transform(y_val.reshape(-1, 1)).flatten(),\n",
    "                ),\n",
    "                epochs=epochs,\n",
    "                batch_size=64,  # Same batch size as ensemble model\n",
    "                callbacks=callbacks,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred_scaled = model.predict(X_val_scaled, verbose=0)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Store results in SAME format as other models\n",
    "            self.models[\"NeuralODE\"] = model\n",
    "            self.results[\"NeuralODE\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val,\n",
    "                \"weights\": val_weights,\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Neural Network\",\n",
    "                \"history\": history,\n",
    "                \"architecture\": \"LSTM+ODE_Blocks+GRU\",  # Added architecture info\n",
    "            }\n",
    "\n",
    "            print(f\"Neural ODE model trained in {training_time:.2f} seconds\")\n",
    "            print(f\"Predictions shape: {y_pred.shape}, Actual shape: {y_val.shape}\")\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Neural ODE training failed: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "            return None, None\n",
    "\n",
    "    def state_space_model(self):\n",
    "        \"\"\"State Space Model using SARIMAX - COMPLETE FIXED VERSION\"\"\"\n",
    "        print(\"=== TRAINING STATE SPACE MODEL (SARIMAX) - COMPLETE FIXED ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Prepare time series data with proper data type handling\n",
    "            ts_data = (\n",
    "                self.train_data.groupby(\"Date\")\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"CPI\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            # Debug: Check for data issues\n",
    "            print(f\"Training data shape before processing: {ts_data.shape}\")\n",
    "            print(f\"Date range: {ts_data['Date'].min()} to {ts_data['Date'].max()}\")\n",
    "\n",
    "            # Check for missing values\n",
    "            missing_counts = ts_data.isnull().sum()\n",
    "            if missing_counts.any():\n",
    "                print(\"Missing values found:\")\n",
    "                print(missing_counts[missing_counts > 0])\n",
    "\n",
    "            # Ensure Date is datetime\n",
    "            ts_data[\"Date\"] = pd.to_datetime(ts_data[\"Date\"])\n",
    "            ts_data.set_index(\"Date\", inplace=True)\n",
    "            ts_data = ts_data.sort_index()\n",
    "\n",
    "            # *** THIS IS THE KEY FIX: USE RESAMPLE INSTEAD OF ASFREQ ***\n",
    "            print(\"Using resample instead of asfreq...\")\n",
    "            ts_data = ts_data.resample(\"W\").agg(\n",
    "                {\n",
    "                    \"Weekly_Sales\": \"sum\",\n",
    "                    \"Temperature\": \"mean\",\n",
    "                    \"Fuel_Price\": \"mean\",\n",
    "                    \"CPI\": \"mean\",\n",
    "                    \"Unemployment\": \"mean\",\n",
    "                    \"IsHoliday\": \"max\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"Shape after resample: {ts_data.shape}\")\n",
    "            print(f\"NaN values after resample: {ts_data.isnull().sum().sum()}\")\n",
    "\n",
    "            # Handle missing values BEFORE data type conversion\n",
    "            if ts_data.isnull().any().any():\n",
    "                print(\"Filling missing values...\")\n",
    "                ts_data = ts_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "                # If still NaN, fill with column medians\n",
    "                for col in ts_data.columns:\n",
    "                    if ts_data[col].isnull().any():\n",
    "                        median_val = ts_data[col].median()\n",
    "                        ts_data[col] = ts_data[col].fillna(median_val)\n",
    "                        print(f\"Filled {col} with median: {median_val}\")\n",
    "\n",
    "            print(f\"Shape after filling: {ts_data.shape}\")\n",
    "\n",
    "            # SAFE data type conversion - DON'T USE errors='coerce'\n",
    "            exog_vars = [\n",
    "                \"Temperature\",\n",
    "                \"Fuel_Price\",\n",
    "                \"CPI\",\n",
    "                \"Unemployment\",\n",
    "                \"IsHoliday\",\n",
    "            ]\n",
    "\n",
    "            print(\"Converting data types safely...\")\n",
    "            for col in [\"Weekly_Sales\"] + exog_vars:\n",
    "                print(f\"Converting {col}: {ts_data[col].dtype} -> float64\")\n",
    "\n",
    "                if col == \"IsHoliday\":\n",
    "                    # Handle boolean column specially\n",
    "                    if ts_data[col].dtype == \"bool\":\n",
    "                        ts_data[col] = ts_data[col].astype(int).astype(\"float64\")\n",
    "                    elif ts_data[col].dtype == \"object\":\n",
    "                        # Convert True/False strings to 1/0\n",
    "                        ts_data[col] = ts_data[col].map(\n",
    "                            {\"True\": 1, \"False\": 0, True: 1, False: 0}\n",
    "                        )\n",
    "                        ts_data[col] = ts_data[col].astype(\"float64\")\n",
    "                    else:\n",
    "                        ts_data[col] = ts_data[col].astype(\"float64\")\n",
    "                else:\n",
    "                    # For numeric columns, ensure they're float\n",
    "                    if pd.api.types.is_numeric_dtype(ts_data[col]):\n",
    "                        ts_data[col] = ts_data[col].astype(\"float64\")\n",
    "                    else:\n",
    "                        # Only use coerce as last resort and fill immediately\n",
    "                        original_count = len(ts_data)\n",
    "                        ts_data[col] = pd.to_numeric(ts_data[col], errors=\"coerce\")\n",
    "                        nan_count = ts_data[col].isnull().sum()\n",
    "\n",
    "                        if nan_count > 0:\n",
    "                            print(\n",
    "                                f\"  WARNING: {nan_count} values converted to NaN in {col}\"\n",
    "                            )\n",
    "                            median_val = ts_data[col].median()\n",
    "                            ts_data[col] = ts_data[col].fillna(median_val)\n",
    "                            print(f\"  Filled with median: {median_val}\")\n",
    "\n",
    "                        ts_data[col] = ts_data[col].astype(\"float64\")\n",
    "\n",
    "            # Final check - should have NO NaNs and same shape\n",
    "            print(f\"Final training shape: {ts_data.shape}\")\n",
    "            print(f\"Final training NaN count: {ts_data.isnull().sum().sum()}\")\n",
    "            print(f\"Training data types: {ts_data.dtypes}\")\n",
    "\n",
    "            if ts_data.empty:\n",
    "                print(\"ERROR: Training data is empty!\")\n",
    "                return None, None\n",
    "\n",
    "            if ts_data.isnull().any().any():\n",
    "                print(\"ERROR: Still have NaN values in training data!\")\n",
    "                print(ts_data.isnull().sum())\n",
    "                return None, None\n",
    "\n",
    "            # Verify we have enough data\n",
    "            if len(ts_data) < 10:\n",
    "                print(\"ERROR: Insufficient training data for SARIMAX model\")\n",
    "                return None, None\n",
    "\n",
    "            # Fit SARIMAX model with error handling\n",
    "            try:\n",
    "                model = SARIMAX(\n",
    "                    ts_data[\"Weekly_Sales\"].values,  # Convert to numpy array explicitly\n",
    "                    exog=ts_data[exog_vars].values,  # Convert to numpy array explicitly\n",
    "                    order=(1, 1, 1),  # ARIMA order\n",
    "                    seasonal_order=(1, 1, 1, 52),  # Seasonal order (weekly seasonality)\n",
    "                    enforce_stationarity=False,\n",
    "                    enforce_invertibility=False,\n",
    "                )\n",
    "\n",
    "                print(\"Fitting SARIMAX model...\")\n",
    "                fitted_model = model.fit(disp=False, maxiter=100)\n",
    "\n",
    "            except Exception as model_error:\n",
    "                print(\n",
    "                    f\"SARIMAX fitting failed with seasonal order, trying simpler model: {model_error}\"\n",
    "                )\n",
    "                # Try simpler model without seasonal component\n",
    "                model = SARIMAX(\n",
    "                    ts_data[\"Weekly_Sales\"].values,\n",
    "                    exog=ts_data[exog_vars].values,\n",
    "                    order=(1, 1, 1),  # ARIMA order only\n",
    "                    enforce_stationarity=False,\n",
    "                    enforce_invertibility=False,\n",
    "                )\n",
    "                fitted_model = model.fit(disp=False, maxiter=100)\n",
    "\n",
    "            # Prepare validation data with same processing\n",
    "            val_ts_data = (\n",
    "                self.val_data.groupby(\"Date\")\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        \"Temperature\": \"mean\",\n",
    "                        \"Fuel_Price\": \"mean\",\n",
    "                        \"CPI\": \"mean\",\n",
    "                        \"Unemployment\": \"mean\",\n",
    "                        \"IsHoliday\": \"max\",\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            print(f\"Validation data shape before processing: {val_ts_data.shape}\")\n",
    "\n",
    "            # Process validation data - USE SAME SAFE APPROACH AS TRAINING\n",
    "            val_ts_data[\"Date\"] = pd.to_datetime(val_ts_data[\"Date\"])\n",
    "            val_ts_data.set_index(\"Date\", inplace=True)\n",
    "            val_ts_data = val_ts_data.sort_index()\n",
    "\n",
    "            # *** USE RESAMPLE FOR VALIDATION TOO ***\n",
    "            print(\"Using resample for validation data...\")\n",
    "            val_ts_data = val_ts_data.resample(\"W\").agg(\n",
    "                {\n",
    "                    \"Weekly_Sales\": \"sum\",\n",
    "                    \"Temperature\": \"mean\",\n",
    "                    \"Fuel_Price\": \"mean\",\n",
    "                    \"CPI\": \"mean\",\n",
    "                    \"Unemployment\": \"mean\",\n",
    "                    \"IsHoliday\": \"max\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"Validation shape after resample: {val_ts_data.shape}\")\n",
    "            print(\n",
    "                f\"Validation NaN values after resample: {val_ts_data.isnull().sum().sum()}\"\n",
    "            )\n",
    "\n",
    "            # Handle missing values in validation data BEFORE type conversion\n",
    "            if val_ts_data.isnull().any().any():\n",
    "                print(\"Filling validation missing values...\")\n",
    "                val_ts_data = val_ts_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "                # If still NaN, fill with column medians\n",
    "                for col in val_ts_data.columns:\n",
    "                    if val_ts_data[col].isnull().any():\n",
    "                        median_val = val_ts_data[col].median()\n",
    "                        val_ts_data[col] = val_ts_data[col].fillna(median_val)\n",
    "                        print(f\"Filled validation {col} with median: {median_val}\")\n",
    "\n",
    "            # SAFE data type conversion for validation - DON'T USE errors='coerce'\n",
    "            print(\"Converting validation data types safely...\")\n",
    "            for col in [\"Weekly_Sales\"] + exog_vars:\n",
    "                print(\n",
    "                    f\"Converting validation {col}: {val_ts_data[col].dtype} -> float64\"\n",
    "                )\n",
    "\n",
    "                if col == \"IsHoliday\":\n",
    "                    # Handle boolean column specially\n",
    "                    if val_ts_data[col].dtype == \"bool\":\n",
    "                        val_ts_data[col] = (\n",
    "                            val_ts_data[col].astype(int).astype(\"float64\")\n",
    "                        )\n",
    "                    elif val_ts_data[col].dtype == \"object\":\n",
    "                        # Convert True/False strings to 1/0\n",
    "                        val_ts_data[col] = val_ts_data[col].map(\n",
    "                            {\"True\": 1, \"False\": 0, True: 1, False: 0}\n",
    "                        )\n",
    "                        val_ts_data[col] = val_ts_data[col].astype(\"float64\")\n",
    "                    else:\n",
    "                        val_ts_data[col] = val_ts_data[col].astype(\"float64\")\n",
    "                else:\n",
    "                    # For numeric columns, ensure they're float\n",
    "                    if pd.api.types.is_numeric_dtype(val_ts_data[col]):\n",
    "                        val_ts_data[col] = val_ts_data[col].astype(\"float64\")\n",
    "                    else:\n",
    "                        # Only use coerce as last resort and fill immediately\n",
    "                        val_ts_data[col] = pd.to_numeric(\n",
    "                            val_ts_data[col], errors=\"coerce\"\n",
    "                        )\n",
    "                        nan_count = val_ts_data[col].isnull().sum()\n",
    "\n",
    "                        if nan_count > 0:\n",
    "                            print(\n",
    "                                f\"  WARNING: {nan_count} validation values converted to NaN in {col}\"\n",
    "                            )\n",
    "                            median_val = val_ts_data[col].median()\n",
    "                            val_ts_data[col] = val_ts_data[col].fillna(median_val)\n",
    "                            print(f\"  Filled with median: {median_val}\")\n",
    "\n",
    "                        val_ts_data[col] = val_ts_data[col].astype(\"float64\")\n",
    "\n",
    "            # Final validation check\n",
    "            print(f\"Final validation shape: {val_ts_data.shape}\")\n",
    "            print(f\"Final validation NaN count: {val_ts_data.isnull().sum().sum()}\")\n",
    "\n",
    "            if val_ts_data.empty:\n",
    "                print(\"ERROR: Validation data is empty!\")\n",
    "                return None, None\n",
    "\n",
    "            if val_ts_data.isnull().any().any():\n",
    "                print(\"ERROR: Still have NaN values in validation data!\")\n",
    "                print(val_ts_data.isnull().sum())\n",
    "                return None, None\n",
    "\n",
    "            # Make predictions\n",
    "            forecast_steps = len(val_ts_data)\n",
    "            if forecast_steps == 0:\n",
    "                print(\"ERROR: No validation data available\")\n",
    "                return None, None\n",
    "\n",
    "            print(f\"Forecasting {forecast_steps} steps...\")\n",
    "\n",
    "            # Ensure exogenous variables are numeric arrays\n",
    "            exog_forecast = val_ts_data[exog_vars].values.astype(\"float64\")\n",
    "\n",
    "            # Check for any issues with exogenous data\n",
    "            if np.isnan(exog_forecast).any():\n",
    "                print(\"WARNING: NaN values in exogenous forecast data\")\n",
    "                return None, None\n",
    "\n",
    "            forecast = fitted_model.forecast(steps=forecast_steps, exog=exog_forecast)\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Store results\n",
    "            self.models[\"SARIMAX\"] = fitted_model\n",
    "            self.results[\"SARIMAX\"] = {\n",
    "                \"predictions\": (\n",
    "                    forecast.values if hasattr(forecast, \"values\") else forecast\n",
    "                ),\n",
    "                \"actual\": val_ts_data[\"Weekly_Sales\"].values,\n",
    "                \"weights\": np.ones(len(forecast)),\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Statistical\",\n",
    "                \"model_summary\": str(\n",
    "                    fitted_model.summary()\n",
    "                ),  # Convert to string to avoid issues\n",
    "            }\n",
    "\n",
    "            print(f\"SARIMAX model trained successfully in {training_time:.2f} seconds\")\n",
    "            print(f\"Forecast shape: {forecast.shape}\")\n",
    "            print(f\"Actual shape: {val_ts_data['Weekly_Sales'].values.shape}\")\n",
    "\n",
    "            return fitted_model, (\n",
    "                forecast.values if hasattr(forecast, \"values\") else forecast\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"SARIMAX training failed: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "            return None, None\n",
    "\n",
    "    def gaussian_process_model(self):\n",
    "        \"\"\"Gaussian Process for time series forecasting (simplified using sklearn)\"\"\"\n",
    "        print(\"=== TRAINING GAUSSIAN PROCESS MODEL ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "            from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern\n",
    "\n",
    "            # Prepare data\n",
    "            features = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\", \"IsHoliday\"]\n",
    "\n",
    "            # Aggregate by date\n",
    "            train_agg = (\n",
    "                self.train_data.groupby(\"Date\")\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        **{\n",
    "                            feat: \"mean\" if feat != \"IsHoliday\" else \"max\"\n",
    "                            for feat in features\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            val_agg = (\n",
    "                self.val_data.groupby(\"Date\")\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"Weekly_Sales\": \"sum\",\n",
    "                        **{\n",
    "                            feat: \"mean\" if feat != \"IsHoliday\" else \"max\"\n",
    "                            for feat in features\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            # Prepare features and target\n",
    "            X_train = train_agg[features].values\n",
    "            y_train = train_agg[\"Weekly_Sales\"].values\n",
    "            X_val = val_agg[features].values\n",
    "            y_val = val_agg[\"Weekly_Sales\"].values\n",
    "\n",
    "            # Scale features\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "            X_val_scaled = scaler_X.transform(X_val)\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Define kernel\n",
    "            kernel = Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "\n",
    "            # Create and fit GP model\n",
    "            model = GaussianProcessRegressor(\n",
    "                kernel=kernel,\n",
    "                alpha=1e-6,\n",
    "                normalize_y=False,\n",
    "                n_restarts_optimizer=3,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            # Fit model (subsample for computational efficiency)\n",
    "            if len(X_train_scaled) > 200:\n",
    "                indices = np.random.choice(len(X_train_scaled), 200, replace=False)\n",
    "                X_train_sub = X_train_scaled[indices]\n",
    "                y_train_sub = y_train_scaled[indices]\n",
    "            else:\n",
    "                X_train_sub = X_train_scaled\n",
    "                y_train_sub = y_train_scaled\n",
    "\n",
    "            model.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "            # Make predictions with uncertainty\n",
    "            y_pred_scaled, y_std_scaled = model.predict(X_val_scaled, return_std=True)\n",
    "\n",
    "            # Denormalize\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "            y_std = y_std_scaled * scaler_y.scale_[0]\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            self.models[\"GaussianProcess\"] = model\n",
    "            self.results[\"GaussianProcess\"] = {\n",
    "                \"predictions\": y_pred,\n",
    "                \"actual\": y_val,\n",
    "                \"uncertainty\": y_std,\n",
    "                \"weights\": np.ones(len(y_pred)),\n",
    "                \"training_time\": training_time,\n",
    "                \"model_type\": \"Probabilistic\",\n",
    "            }\n",
    "\n",
    "            print(f\"Gaussian Process model trained in {training_time:.2f} seconds\")\n",
    "            return model, y_pred\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Gaussian Process training failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "# Usage example and test functions\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from src.data_loader import WalmartDataLoader\n",
    "    from src.data_processing import WalmartComprehensiveEDA\n",
    "    from src.feature_engineering import WalmartFeatureEngineering\n",
    "\n",
    "    data_loader = WalmartDataLoader()\n",
    "    data_loader.load_data()\n",
    "    train_data = data_loader.train_data\n",
    "    test_data = data_loader.test_data\n",
    "    features_data = data_loader.features_data\n",
    "    stores_data = data_loader.stores_data\n",
    "\n",
    "    # Assuming you have data from WalmartDataLoader\n",
    "    eda = WalmartComprehensiveEDA(train_data, test_data, features_data, stores_data)\n",
    "    merged_data = eda.merge_datasets()\n",
    "\n",
    "    feature_eng = WalmartFeatureEngineering(merged_data)\n",
    "    processed_data = feature_eng.create_walmart_features()\n",
    "    processed_data = feature_eng.handle_missing_values()\n",
    "    print(\"Feature Engineering class ready!\")\n",
    "\n",
    "    # Example usage:\n",
    "    advanced_models = AdvancedWalmartForecastingModels(processed_data)\n",
    "    train_data, val_data = advanced_models.prepare_data()\n",
    "\n",
    "    # Train various models\n",
    "    tft_model, tft_pred = advanced_models.temporal_fusion_transformer_advanced()\n",
    "    ensemble_model, ensemble_pred = advanced_models.ensemble_deep_learning_model()\n",
    "    ode_model, ode_pred = advanced_models.neural_ode_model()\n",
    "    sarimax_model, sarimax_pred = advanced_models.state_space_model()\n",
    "    gp_model, gp_pred = advanced_models.gaussian_process_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d03ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TFT_Advanced', 'EnsembleDeep', 'NeuralODE', 'SARIMAX', 'GaussianProcess'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_models.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc22a274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1468960.8 , 1568785.5 , 1401252.8 , 1536595.4 , 1534727.9 ,\n",
       "       1527413.6 ,  531471.8 ,  523890.3 ,  523479.12, 2097619.5 ,\n",
       "       2079817.2 , 2051390.6 ,  525402.25,  518914.25,  519263.7 ,\n",
       "       1550326.  , 1528853.6 , 1508155.6 ,  667022.9 ,  670484.3 ,\n",
       "        658965.06,  796802.8 ,  817215.6 ,  801260.7 ,  633969.1 ,\n",
       "        635364.  ,  631166.3 , 1730265.6 , 1784494.6 , 1750054.4 ,\n",
       "       1538699.  , 1536218.  , 1530461.5 , 1029654.25, 1018852.3 ,\n",
       "       1020372.  , 2046897.2 , 2028255.2 , 2015602.  , 1664766.  ,\n",
       "       1680090.1 , 1683354.9 ,  967529.7 ,  939231.4 ,  932945.44,\n",
       "        580742.3 ,  582540.44,  580036.25,  981530.1 ,  992248.2 ,\n",
       "       1002800.75, 1032213.56, 1013025.06,  979147.25, 1596465.6 ,\n",
       "       1588624.  , 1566017.2 , 1669508.1 , 1718824.2 , 1728527.2 ,\n",
       "        841552.4 ,  846652.56,  836320.2 , 1011496.25,  988235.8 ,\n",
       "        977094.6 , 1329710.6 , 1328042.9 , 1330569.1 , 1556516.8 ,\n",
       "       1539797.  , 1536859.8 ,  836382.25,  821295.6 ,  811896.25,\n",
       "       1023859.56, 1019536.6 , 1009795.7 , 1821159.8 , 1792075.8 ,\n",
       "       1768368.  , 1399290.2 , 1371247.4 , 1383427.1 ,  692917.44,\n",
       "        693907.25,  695829.2 ,  541771.  ,  533085.75,  532511.94,\n",
       "       1548804.2 , 1551849.5 , 1541850.9 , 1592995.6 , 1605775.2 ,\n",
       "       1620231.6 ,  530910.25,  526941.7 ,  531230.4 , 1010457.9 ,\n",
       "       1011378.25, 1013032.1 ,  878615.9 ,  854321.3 ,  873523.06,\n",
       "        537540.7 ,  529484.5 ,  528633.1 ,  537589.4 ,  529422.6 ,\n",
       "        528636.1 ,  531529.5 ,  526730.6 ,  527193.1 , 1580349.4 ,\n",
       "       1534935.  , 1523737.8 ,  991354.2 ,  987804.56,  984779.3 ,\n",
       "       1530442.8 , 1539479.4 , 1539031.2 ,  529869.9 ,  527720.7 ,\n",
       "        531594.44,  675029.9 ,  667876.9 ,  665161.56,  539624.2 ,\n",
       "        538130.3 ,  540718.7 ,  858438.4 ,  856987.06,  841582.2 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_models.results[\"TFT_Advanced\"][\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620ddbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1573072.81, 1508068.77, 1493659.74, 1900745.13, 1847990.41,\n",
       "       1834458.35,  410804.39,  424513.08,  405432.7 , 2133026.07,\n",
       "       2097266.85, 2149594.46,  325345.41,  313358.15,  319550.77,\n",
       "       1459396.84, 1436883.99, 1431426.34,  503463.93,  516424.83,\n",
       "        495543.28,  927511.99,  900309.75,  891671.44,  558464.8 ,\n",
       "        542009.46,  549731.49, 1713889.11, 1734834.82, 1744349.05,\n",
       "       1311965.09, 1232073.18, 1200729.45,  934917.47,  960945.43,\n",
       "        974697.6 , 1999079.44, 2018010.15, 2035189.66, 1639585.61,\n",
       "       1590274.72, 1704357.62,  551799.63,  555652.77,  558473.6 ,\n",
       "        491817.19,  577198.97,  475770.14,  919878.34,  957356.84,\n",
       "        943465.29, 1074079.  , 1048706.75, 1127516.25, 1352809.5 ,\n",
       "       1321102.35, 1322117.96, 2162951.36, 1999363.49, 2031650.55,\n",
       "        653043.44,  641368.14,  675202.87, 1004039.84,  978027.95,\n",
       "       1094422.69, 1412925.25, 1363155.77, 1347454.59, 1416301.17,\n",
       "       1255414.84, 1307182.29,  697317.41,  685531.85,  688940.94,\n",
       "       1044639.69,  975578.02,  958619.8 , 1660081.29, 1620374.24,\n",
       "       1703047.74, 1205536.71, 1143724.48, 1213860.61,  513737.  ,\n",
       "        516909.24,  534970.68,  434593.26,  437537.29,  439424.5 ,\n",
       "       1401113.42, 1378730.45, 1340232.55, 1176681.31, 1199292.06,\n",
       "       1219979.29,  291781.15,  254412.34,  253731.13,  948613.39,\n",
       "        963516.28,  956987.81,  873643.14,  829284.67,  865137.6 ,\n",
       "        300236.85,  287360.05,  272489.41,  521810.75,  551969.1 ,\n",
       "        534738.43,  437320.66,  428806.46,  417290.38, 1494417.07,\n",
       "       1577486.33, 1569502.  ,  982523.26,  918170.5 ,  921264.52,\n",
       "       1409544.97, 1326197.24, 1316542.59,  612379.9 ,  541406.98,\n",
       "        514756.08,  619369.72,  623919.23,  587603.55,  337796.13,\n",
       "        323766.77,  361067.07,  734464.36,  718125.53,  760281.43])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_models.results[\"TFT_Advanced\"][\"actual\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banking_ts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
